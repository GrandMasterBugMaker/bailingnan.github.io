<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>白凌南</title>
<meta name="description" content="DL/RecSys/Python/Java/INTJ" />
<link rel="shortcut icon" href="https://bailingnan.github.io//favicon.ico?v=1580407924678">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link href="https://cdn.remixicon.com/releases/v1.3.1/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.2/animate.min.css">

<link rel="stylesheet" href="https://bailingnan.github.io//styles/main.css">
<link rel="alternate" type="application/atom+xml" title="白凌南 - Atom Feed" href="https://bailingnan.github.io//atom.xml">



  </head>
  <body>
    <div id="app" class="main px-4 flex flex-col lg:flex-row">
      <div id="sidebar" class="sidebar-wrapper lg:static lg:w-1/4">
  <div class="lg:sticky top-0">
    <div class="sidebar-content">
      <div class="flex lg:block p-4 lg:px-0 items-center fixed lg:static lg:block top-0 right-0 left-0 bg-white z-50">
        <i class="remixicon-menu-2-line lg:mt-4 text-2xl cursor-pointer animated fadeIn" onclick="openMenu()"></i>
        <a href="https://bailingnan.github.io/">
          <img class="animated fadeInLeft avatar rounded-lg mx-4 lg:mt-32 lg:mx-0 mt-0 lg:w-24 lg:h-24 w-12 w-12" src="https://bailingnan.github.io//images/avatar.png?v=1580407924678" alt="">
        </a>
        <h1 class="animated fadeInLeft lg:text-4xl font-extrabold lg:mt-8 mt-0 text-xl" style="animation-delay: 0.2s">白凌南</h1>
      </div>
      
        <div class="animated fadeInLeft" style="animation-delay: 0.4s">
          <p class="my-4 text-gray-600 font-light hidden lg:block">
            DL/RecSys/Python/Java/INTJ
          </p>
          <div class="animated fadeInLeft social-container hidden lg:block" style="animation-delay: 0.6s">
            
              
                <a href="https://github.com/bailingnan" target="_blank" class="mr-4 text-xl text-gray-400 font-light hover:text-gray-900">
                  <i class="remixicon-github-line"></i>
                </a>
              
            
              
            
              
                <a href="https://www.weibo.com/buggerking" target="_blank" class="mr-4 text-xl text-gray-400 font-light hover:text-gray-900">
                  <i class="remixicon-weibo-line"></i>
                </a>
              
            
              
                <a href="https://www.zhihu.com/people/bailingnan" target="_blank" class="mr-4 text-xl text-gray-400 font-light hover:text-gray-900">
                  <i class="remixicon-zhihu-line"></i>
                </a>
              
            
              
            
              
            
              
            
              
            
              
            
              
            
          </div>
        </div>
      
    </div>
  </div>
</div>

<div class="menu-container">
  <i class="remixicon-arrow-left-line text-2xl cursor-pointer animated fadeIn close-menu-btn" onclick="closeMenu()"></i>
  <div>
    
      
        <a href="/" class="menu" style="animation-delay: 0s">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu" style="animation-delay: 0.2s">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu" style="animation-delay: 0.4s">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu" style="animation-delay: 0.6000000000000001s">
          关于
        </a>
      
    
  </div>
  <div class="site-footer">
    <div class="py-4 text-gray-700"></div>
    <a class="rss" href="https://bailingnan.github.io//atom.xml" target="_blank">RSS</a>
  </div>
</div>
<div class="mask" onclick="closeMenu()">
</div>
      <div class="content-wrapper py-32 lg:p-8 lg:w-3/4">
        <h2 class="text-lg text-gray-700 mb-8">
          <i class="remixicon-bookmark-line"></i> PyTorch
        </h2>
        
  <section class="post-item md:flex pb-12 animated fadeIn" :style="{ 'animation-delay': `${0 * 0.2}s` }">
    
    <div class="content">
      <a href="https://bailingnan.github.io/post/pytorch-chang-yong-dai-ma-duan">
        <h2 class="post-title text-xl font-extrabold mt-5 md:mt-0">PyTorch常用代码段</h2>
      </a>
      <div class="post-abstract text-gray-700 font-light my-4">
        <h1 id="1基本配置">1.基本配置</h1>
<h2 id="导入包和版本查询">导入包和版本查询</h2>
<pre><code class="language-Python">import torch
import torch.nn as nn
import torchvision
print(torch.__version__)# PyTorch version
print(torch.version.cuda)#Corresponding CUDA version
print(torch.backends.cudnn.version())#Corresponding cuDNN version
print(torch.cuda.get_device_name(0))#GPU type
</code></pre>
<h2 id="可复现性">可复现性</h2>
<p>在硬件设备（CPU、GPU）不同时，完全的可复现性无法保证，即使随机种子相同。但是，在同一个设备上，应该保证可复现性。具体做法是，在程序开始的时候固定torch的随机种子，同时也把numpy的随机种子固定。</p>
<pre><code class="language-Python">np.random.seed(0)
torch.manual_seed(0)
torch.cuda.manual_seed_all(0)
</code></pre>
<h2 id="显卡设置">显卡设置</h2>
<p>如果只需要一张显卡</p>
<pre><code class="language-Python"># Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
</code></pre>
<p>如果需要指定多张显卡，比如0，1号显卡。</p>
<pre><code class="language-Python">import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
</code></pre>
<p>也可以在命令行运行代码时设置显卡：</p>
<pre><code>CUDA_VISIBLE_DEVICES=0,1 python train.py
</code></pre>
<p>清除显存:</p>
<pre><code class="language-Python">torch.cuda.empty_cache()
</code></pre>
<p>也可以使用在命令行重置GPU的指令：</p>
<pre><code>nvidia-smi --gpu-reset -i [gpu_id]
</code></pre>
<p>或在命令行可以先使用ps找到程序的PID，再使用kill结束该进程</p>
<pre><code class="language-python">ps aux | grep python
kill -9 [pid]
</code></pre>
<h2 id="设置为cudnn-benchmark模式">设置为cuDNN benchmark模式</h2>
<p>Benchmark模式会提升计算速度，但是由于计算中有随机性，每次网络前馈结果略有差异。</p>
<pre><code class="language-python">torch.backends.cudnn.benchmark = True
</code></pre>
<p>如果想要避免这种结果波动，设置</p>
<pre><code class="language-python">torch.backends.cudnn.deterministic = True
</code></pre>
<h1 id="张量tensor处理">张量(Tensor)处理</h1>
<h2 id="张量基本信息">张量基本信息</h2>
<pre><code class="language-Python">tensor = torch.randn(3,4,5)
print(tensor.type())  # 数据类型
print(tensor.size())  # 张量的shape，是个元组
print(tensor.dim())   # 维度的数量
</code></pre>
<h2 id="命名变量">命名变量</h2>
<pre><code class="language-Python"># 在PyTorch 1.3之前，需要使用注释
# Tensor[N, C, H, W]
images = torch.randn(32, 3, 56, 56)
images.sum(dim=1)
images.select(dim=1, index=0)

# PyTorch 1.3之后
NCHW = [‘N’, ‘C’, ‘H’, ‘W’]
images = torch.randn(32, 3, 56, 56, names=NCHW)
images.sum('C')
images.select('C', index=0)
# 也可以这么设置
tensor = torch.rand(3,4,1,2,names=('C', 'N', 'H', 'W'))
# 使用align_to可以对维度方便地排序
tensor = tensor.align_to('N', 'C', 'H', 'W')
</code></pre>
<h2 id="数据类型转换">数据类型转换</h2>
<pre><code class="language-Python"># 设置默认类型，pytorch中的FloatTensor远远快于DoubleTensor
torch.set_default_tensor_type(torch.FloatTensor)

# 类型转换
tensor = tensor.cuda()
tensor = tensor.cpu()
tensor = tensor.float()
tensor = tensor.long()
</code></pre>
<h2 id="torchtensor与npndarray转换">torch.Tensor与np.ndarray转换</h2>
<p>除了CharTensor，其他所有CPU上的张量都支持转换为numpy格式然后再转换回来。</p>
<pre><code class="language-Python">ndarray = tensor.cpu().numpy()
tensor = torch.from_numpy(ndarray).float()
tensor = torch.from_numpy(ndarray.copy()).float() # If ndarray has negative stride.
</code></pre>
<h2 id="从只包含一个元素的张量中提取值">从只包含一个元素的张量中提取值</h2>
<p><code>value = torch.rand(1).item()</code></p>
<h2 id="张量形变">张量形变</h2>
<pre><code class="language-Python"># 在将卷积层输入全连接层的情况下通常需要对张量做形变处理，
# 相比torch.view，torch.reshape可以自动处理输入张量不连续的情况。
tensor = torch.rand(2,3,4)
shape = (6, 4)
tensor = torch.reshape(tensor, shape)
</code></pre>
<h2 id="打乱顺序">打乱顺序</h2>
<pre><code class="language-Python"># Operation                 |  New/Shared memory | Still in computation graph |
tensor.clone()            # |        New         |          Yes               |
tensor.detach()           # |      Shared        |          No                |
tensor.detach.clone()()   # |        New         |          No                |
</code></pre>
<h2 id="张量拼接">张量拼接</h2>
<pre><code class="language-Python">'''
注意torch.cat和torch.stack的区别在于torch.cat沿着给定的维度拼接，
而torch.stack会新增一维。例如当参数是3个10x5的张量，torch.cat的结果是30x5的张量，
而torch.stack的结果是3x10x5的张量。
'''
tensor = torch.cat(list_of_tensors, dim=0)
tensor = torch.stack(list_of_tensors, dim=0)
</code></pre>
<h2 id="将整数标签转为one-hot编码">将整数标签转为one-hot编码</h2>
<pre><code class="language-python"># pytorch的标记默认从0开始
tensor = torch.tensor([0, 2, 1, 3])
N = tensor.size(0)
num_classes = 4
one_hot = torch.zeros(N, num_classes).long()
one_hot.scatter_(dim=1, index=torch.unsqueeze(tensor, dim=1), src=torch.ones(N, num_classes).long())
</code></pre>
<h2 id="得到非零元素">得到非零元素</h2>
<pre><code class="language-python">torch.nonzero(tensor)               # index of non-zero elements,包含点坐标的列表向量
torch.nonzero(tensor==0)            # index of zero elements
torch.nonzero(tensor).size(0)       # number of non-zero elements
torch.nonzero(tensor == 0).size(0)  # number of zero elements
</code></pre>
<h2 id="判断两个张量相等">判断两个张量相等</h2>
<pre><code class="language-Python">torch.allclose(tensor1, tensor2)  # float tensor
torch.equal(tensor1, tensor2)     # int tensor
</code></pre>
<h2 id="张量扩展">张量扩展</h2>
<pre><code class="language-python"># Expand tensor of shape 64*512 to shape 64*512*7*7.
tensor = torch.rand(64,512)
torch.reshape(tensor, (64, 512, 1, 1)).expand(64, 512, 7, 7)
</code></pre>
<h2 id="矩阵乘法">矩阵乘法</h2>
<pre><code class="language-python"># Matrix multiplcation: (m*n) * (n*p) * -&gt; (m*p).
result = torch.mm(tensor1, tensor2)

# Batch matrix multiplication: (b*m*n) * (b*n*p) -&gt; (b*m*p)
result = torch.bmm(tensor1, tensor2)

# Element-wise multiplication.
result = tensor1 * tensor2
</code></pre>
<h2 id="计算两组数据之间的两两欧式距离">计算两组数据之间的两两欧式距离</h2>
<p>利用broadcast机制</p>
<pre><code class="language-python">dist = torch.sqrt(torch.sum((X1[:,None,:] - X2) ** 2, dim=2))
</code></pre>
<h1 id="模型定义和操作">模型定义和操作</h1>
<h2 id="计算模型整体参数量">计算模型整体参数量</h2>
<pre><code class="language-python">num_parameters = sum(torch.numel(parameter) for parameter in model.parameters())
</code></pre>
<h2 id="查看网络中的参数">查看网络中的参数</h2>
<p>可以通过model.state_dict()或者model.named_parameters()函数查看现在的全部可训练参数（包括通过继承得到的父类中的参数）</p>
<pre><code class="language-python">params = list(model.named_parameters())
(name, param) = params[28]
print(name)
print(param.grad)
print('-------------------------------------------------')
(name2, param2) = params[29]
print(name2)
print(param2.grad)
print('----------------------------------------------------')
(name1, param1) = params[30]
print(name1)
print(param1.grad)
</code></pre>
<h2 id="类似-keras-的-modelsummary-输出模型信息使用pytorch-summary">类似 Keras 的 model.summary() 输出模型信息（使用pytorch-summary ）</h2>
<h2 id="模型权重初始化">模型权重初始化</h2>
<p>注意 model.modules() 和 model.children() 的区别：model.modules() 会迭代地遍历模型的所有子层，而 model.children() 只会遍历模型下的一层。</p>
<pre><code class="language-python"># Common practise for initialization.
for layer in model.modules():
    if isinstance(layer, torch.nn.Conv2d):
        torch.nn.init.kaiming_normal_(layer.weight, mode='fan_out',
                                      nonlinearity='relu')
        if layer.bias is not None:
            torch.nn.init.constant_(layer.bias, val=0.0)
    elif isinstance(layer, torch.nn.BatchNorm2d):
        torch.nn.init.constant_(layer.weight, val=1.0)
        torch.nn.init.constant_(layer.bias, val=0.0)
    elif isinstance(layer, torch.nn.Linear):
        torch.nn.init.xavier_normal_(layer.weight)
        if layer.bias is not None:
            torch.nn.init.constant_(layer.bias, val=0.0)

# Initialization with given tensor.
layer.weight = torch.nn.Parameter(tensor)
</code></pre>
<h2 id="提取模型中的某一层">提取模型中的某一层</h2>
<p>modules()会返回模型中所有模块的迭代器，它能够访问到最内层，比如self.layer1.conv1这个模块，还有一个与它们相对应的是name_children()属性以及named_modules(),这两个不仅会返回模块的迭代器，还会返回网络层的名字。</p>
<pre><code class="language-python"># 取模型中的前两层
new_model = nn.Sequential(*list(model.children())[:2] 
# 如果希望提取出模型中的所有卷积层，可以像下面这样操作：
for layer in model.named_modules():
    if isinstance(layer[1],nn.Conv2d):
         conv_model.add_module(layer[0],layer[1])
</code></pre>
<h2 id="部分层使用预训练模型">部分层使用预训练模型</h2>
<p>注意如果保存的模型是 torch.nn.DataParallel，则当前的模型也需要是</p>
<pre><code class="language-python">model.load_state_dict(torch.load('model.pth'), strict=False)
</code></pre>
<h1 id="模型训练和测试">模型训练和测试</h1>
<h2 id="自定义loss">自定义loss</h2>
<p>继承torch.nn.Module类写自己的loss。</p>
<pre><code class="language-python">class MyLoss(torch.nn.Moudle):
    def __init__(self):
        super(MyLoss, self).__init__()
        
    def forward(self, x, y):
        loss = torch.mean((x - y) ** 2)
        return loss
</code></pre>
<h2 id="标签平滑label-smoothing">标签平滑（label smoothing）</h2>
<p>写一个label_smoothing.py的文件，然后在训练代码里引用，用LSR代替交叉熵损失即可。label_smoothing.py内容如下：</p>
<pre><code class="language-python">import torch
import torch.nn as nn


class LSR(nn.Module):

    def __init__(self, e=0.1, reduction='mean'):
        super().__init__()

        self.log_softmax = nn.LogSoftmax(dim=1)
        self.e = e
        self.reduction = reduction
    
    def _one_hot(self, labels, classes, value=1):
        &quot;&quot;&quot;
            Convert labels to one hot vectors
        
        Args:
            labels: torch tensor in format [label1, label2, label3, ...]
            classes: int, number of classes
            value: label value in one hot vector, default to 1
        
        Returns:
            return one hot format labels in shape [batchsize, classes]
        &quot;&quot;&quot;

        one_hot = torch.zeros(labels.size(0), classes)

        #labels and value_added  size must match
        labels = labels.view(labels.size(0), -1)
        value_added = torch.Tensor(labels.size(0), 1).fill_(value)

        value_added = value_added.to(labels.device)
        one_hot = one_hot.to(labels.device)

        one_hot.scatter_add_(1, labels, value_added)

        return one_hot

    def _smooth_label(self, target, length, smooth_factor):
        &quot;&quot;&quot;convert targets to one-hot format, and smooth
        them.
        Args:
            target: target in form with [label1, label2, label_batchsize]
            length: length of one-hot format(number of classes)
            smooth_factor: smooth factor for label smooth
        
        Returns:
            smoothed labels in one hot format
        &quot;&quot;&quot;
        one_hot = self._one_hot(target, length, value=1 - smooth_factor)
        one_hot += smooth_factor / (length - 1)

        return one_hot.to(target.device)

    def forward(self, x, target):

        if x.size(0) != target.size(0):
            raise ValueError('Expected input batchsize ({}) to match target batch_size({})'
                    .format(x.size(0), target.size(0)))

        if x.dim() &lt; 2:
            raise ValueError('Expected input tensor to have least 2 dimensions(got {})'
                    .format(x.size(0)))

        if x.dim() != 2:
            raise ValueError('Only 2 dimension tensor are implemented, (got {})'
                    .format(x.size()))


        smoothed_target = self._smooth_label(target, x.size(1), self.e)
        x = self.log_softmax(x)
        loss = torch.sum(- x * smoothed_target, dim=1)

        if self.reduction == 'none':
            return loss
        
        elif self.reduction == 'sum':
            return torch.sum(loss)
        
        elif self.reduction == 'mean':
            return torch.mean(loss)
        
        else:
            raise ValueError('unrecognized option, expect reduction to be one of none, mean, sum')
</code></pre>
<p>或者直接在训练文件里做label smoothing</p>
<pre><code class="language-python">for images, labels in train_loader:
    images, labels = images.cuda(), labels.cuda()
    N = labels.size(0)
    # C is the number of classes.
    smoothed_labels = torch.full(size=(N, C), fill_value=0.1 / (C - 1)).cuda()
    smoothed_labels.scatter_(dim=1, index=torch.unsqueeze(labels, dim=1), value=0.9)

    score = model(images)
    log_prob = torch.nn.functional.log_softmax(score, dim=1)
    loss = -torch.sum(log_prob * smoothed_labels) / N
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()    
</code></pre>
<h2 id="l1-正则化">L1 正则化</h2>
<pre><code class="language-python">l1_regularization = torch.nn.L1Loss(reduction='sum')
loss = ...  # Standard cross-entropy loss
for param in model.parameters():
    loss += torch.sum(torch.abs(param))
loss.backward()
</code></pre>
<h2 id="不对偏置项进行权重衰减weight-decay">不对偏置项进行权重衰减（weight decay）</h2>
<p>pytorch里的weight decay相当于l2正则</p>
<pre><code class="language-python">bias_list = (param for name, param in model.named_parameters() if name[-4:] == 'bias')
others_list = (param for name, param in model.named_parameters() if name[-4:] != 'bias')
parameters = [{'parameters': bias_list, 'weight_decay': 0},                
              {'parameters': others_list}]
optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4)
</code></pre>
<h2 id="梯度裁剪gradient-clipping">梯度裁剪（gradient clipping）</h2>
<pre><code class="language-python">torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20)
</code></pre>
<h2 id="得到当前学习率">得到当前学习率</h2>
<pre><code class="language-python"># If there is one global learning rate (which is the common case).
lr = next(iter(optimizer.param_groups))['lr']

# If there are multiple learning rates for different layers.
all_lr = []
for param_group in optimizer.param_groups:
    all_lr.append(param_group['lr'])
</code></pre>
<p>另一种方法，在一个batch训练代码里，当前的lr是</p>
<pre><code class="language-python">optimizer.param_groups[0]['lr']
</code></pre>
<h2 id="学习率衰减">学习率衰减</h2>
<pre><code class="language-python"># Reduce learning rate when validation accuarcy plateau.
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, verbose=True)
for t in range(0, 80):
    train(...)
    val(...)
    scheduler.step(val_acc)

# Cosine annealing learning rate.
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=80)
# Reduce learning rate by 10 at given epochs.
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 70], gamma=0.1)
for t in range(0, 80):
    scheduler.step()    
    train(...)
    val(...)

# Learning rate warmup by 10 epochs.
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: t / 10)
for t in range(0, 10):
    scheduler.step()
    train(...)
    val(...)
</code></pre>
<h2 id="优化器链式更新">优化器链式更新</h2>
<p>从1.4版本开始，<code>torch.optim.lr_scheduler</code> 支持链式更新（chaining），即用户可以定义两个 schedulers，并交替在训练中使用。</p>
<pre><code class="language-python">import torch
from torch.optim import SGD
from torch.optim.lr_scheduler import ExponentialLR, StepLR
model = [torch.nn.Parameter(torch.randn(2, 2, requires_grad=True))]
optimizer = SGD(model, 0.1)
scheduler1 = ExponentialLR(optimizer, gamma=0.9)
scheduler2 = StepLR(optimizer, step_size=3, gamma=0.1)
for epoch in range(4):
    print(epoch, scheduler2.get_last_lr()[0])
    optimizer.step()
    scheduler1.step()
    scheduler2.step()
</code></pre>
<h2 id="模型训练可视化">模型训练可视化</h2>
<p>PyTorch可以使用tensorboard来可视化训练过程。<br>
安装和运行TensorBoard。</p>
<pre><code class="language-python">pip install tensorboard
tensorboard --logdir=runs
</code></pre>
<p>使用SummaryWriter类来收集和可视化相应的数据，放了方便查看，可以使用不同的文件夹，比如'Loss/train'和'Loss/test'。</p>
<pre><code class="language-python">from torch.utils.tensorboard import SummaryWriter
import numpy as np

writer = SummaryWriter()

for n_iter in range(100):
    writer.add_scalar('Loss/train', np.random.random(), n_iter)
    writer.add_scalar('Loss/test', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)
</code></pre>
<h2 id="保存与加载断点">保存与加载断点</h2>
<p>注意为了能够恢复训练，我们需要同时保存模型和优化器的状态，以及当前的训练轮数。</p>
<pre><code class="language-python">start_epoch = 0
# Load checkpoint.
if resume: # resume为参数，第一次训练时设为0，中断再训练时设为1
    model_path = os.path.join('model', 'best_checkpoint.pth.tar')
    assert os.path.isfile(model_path)
    checkpoint = torch.load(model_path)
    best_acc = checkpoint['best_acc']
    start_epoch = checkpoint['epoch']
    model.load_state_dict(checkpoint['model'])
    optimizer.load_state_dict(checkpoint['optimizer'])
    print('Load checkpoint at epoch {}.'.format(start_epoch))
    print('Best accuracy so far {}.'.format(best_acc))

# Train the model
for epoch in range(start_epoch, num_epochs): 
    ... 

    # Test the model
    ...
        
    # save checkpoint
    is_best = current_acc &gt; best_acc
    best_acc = max(current_acc, best_acc)
    checkpoint = {
        'best_acc': best_acc,
        'epoch': epoch + 1,
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict(),
    }
    model_path = os.path.join('model', 'checkpoint.pth.tar')
    best_model_path = os.path.join('model', 'best_checkpoint.pth.tar')
    torch.save(checkpoint, model_path)
    if is_best:
        shutil.copy(model_path, best_model_path)
</code></pre>
<h1 id="其他注意事项">其他注意事项</h1>
<ul>
<li>建议有参数的层和汇合（pooling）层使用<code>torch.nn</code>模块定义，激活函数直接使用<code>torch.nn.functional。torch.nn</code>模块和<code>torch.nn.functional</code>的区别在于，<code>torch.nn</code>模块在计算时底层调用了<code>torch.nn.functional</code>，但<code>torch.nn</code>模块包括该层参数，还可以应对训练和测试两种网络状态。使用<code>torch.nn.functional</code>时要注意网络状态，如</li>
</ul>
<pre><code class="language-python">def forward(self, x):
  ...
  x = torch.nn.functional.dropout(x, p=0.5, training=self.training)
</code></pre>
<ul>
<li>不要使用太大的线性层。因为nn.Linear(m,n)使用的是O(mn)的内存，线性层太大很容易超出现有显存。</li>
<li>不要在太长的序列上使用RNN。因为RNN反向传播使用的是BPTT算法，其需要的内存和输入序列的长度呈线性关系。</li>
<li>model(x) 前用 <code>model.train()</code> 和 <code>model.eval()</code> 切换网络状态。</li>
<li>不需要计算梯度的代码块用 with torch.no_grad() 包含起来。</li>
<li><code>model.eval()</code> 和 <code>torch.no_grad()</code> 的区别在于，<code>model.eval()</code> 是将网络切换为测试状态，例如 BN 和dropout在训练和测试阶段使用不同的计算方法。<code>torch.no_grad()</code>是关闭 PyTorch 张量的自动求导机制，以减少存储使用和加速计算，得到的结果无法进行 <code>loss.backward()</code>。</li>
<li><code>model.zero_grad()</code>会把整个模型的参数的梯度都归零, 而<code>optimizer.zero_grad()</code>只会把传入其中的参数的梯度归零。</li>
<li>torch.nn.CrossEntropyLoss 的输入不需要经过 Softmax。torch.nn.CrossEntropyLoss 等价于 torch.nn.functional.log_softmax + torch.nn.NLLLoss。</li>
<li><code>loss.backward()</code>前用 <code>optimizer.zero_grad()</code> 清除累积梯度。</li>
<li><code>torch.utils.data.DataLoader</code> 中尽量设置 <code>pin_memory=True</code>，对特别小的数据集如 MNIST 设置 <code>pin_memory=False</code> 反而更快一些。<code>num_workers</code> 的设置需要在实验中找到最快的取值。</li>
<li>用 <code>del</code>及时删除不用的中间变量，节约 GPU 存储。</li>
<li>使用 <code>inplace</code> 操作可节约 GPU 存储，如</li>
</ul>
<pre><code class="language-python">x = torch.nn.functional.relu(x, inplace=True)
</code></pre>
<ul>
<li>减少 CPU 和 GPU 之间的数据传输。例如如果你想知道一个 epoch 中每个 mini-batch 的 loss 和准确率，先将它们累积在 GPU 中等一个 epoch 结束之后一起传输回 CPU 会比每个 mini-batch 都进行一次 GPU 到 CPU 的传输更快。</li>
<li>使用半精度浮点数 <code>half()</code>会有一定的速度提升，具体效率依赖于 GPU 型号。需要小心数值精度过低带来的稳定性问题。</li>
<li>时常使用 <code>assert tensor.size() == (N, D, H, W)</code> 作为调试手段，确保张量维度和你设想中一致。</li>
<li>除了标记 y 外，尽量少使用一维张量，使用 n*1 的二维张量代替，可以避免一些意想不到的一维张量计算结果。</li>
<li>统计代码各部分耗时</li>
</ul>
<pre><code class="language-python">with torch.autograd.profiler.profile(enabled=True, use_cuda=False) as profile:
    ...
print(profile)

# 或者在命令行运行
python -m torch.utils.bottleneck main.py
</code></pre>
<ul>
<li>使用TorchSnooper来调试PyTorch代码，程序在执行的时候，就会自动 print 出来每一行的执行结果的 tensor 的形状、数据类型、设备、是否需要梯度的信息。</li>
</ul>
<pre><code class="language-python"># pip install torchsnooper
import torchsnooper

# 对于函数，使用修饰器
@torchsnooper.snoop()

# 如果不是函数，使用 with 语句来激活 TorchSnooper，把训练的那个循环装进 with 语句中去。
with torchsnooper.snoop():
    原本的代码
</code></pre>
<ul>
<li>模型可解释性，使用captum库</li>
</ul>

      </div>
      <div class="text-gray-400 text-sm font-light">2020-01-27 / 17 min read</div>
    </div>
  </section>

        <div class="pagination-container animated fadeIn">
  
  
</div>

      </div>
    </div>

    <script src="https://bailingnan.github.io//media/prism.js"></script>  
<script>

Prism.highlightAll()

let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

// This should probably be throttled.
// Especially because it triggers during smooth scrolling.
// https://lodash.com/docs/4.17.10#throttle
// You could do like...
// window.addEventListener("scroll", () => {
//    _.throttle(doThatStuff, 100);
// });
// Only not doing it here to keep this Pen dependency-free.

window.addEventListener("scroll", event => {
  let fromTop = window.scrollY;

  mainNavLinks.forEach((link, index) => {
    let section = document.getElementById(decodeURI(link.hash).substring(1));
    let nextSection = null
    if (mainNavLinks[index + 1]) {
      nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
    }
    if (section.offsetTop <= fromTop) {
      if (nextSection) {
        if (nextSection.offsetTop > fromTop) {
          link.classList.add("current");
        } else {
          link.classList.remove("current");    
        }
      } else {
        link.classList.add("current");
      }
    } else {
      link.classList.remove("current");
    }
  });
});


document.addEventListener("DOMContentLoaded", function() {
  var lazyImages = [].slice.call(document.querySelectorAll(".post-feature-image.lazy"));

  if ("IntersectionObserver" in window) {
    let lazyImageObserver = new IntersectionObserver(function(entries, observer) {
      entries.forEach(function(entry) {
        if (entry.isIntersecting) {
          let lazyImage = entry.target
          lazyImage.style.backgroundImage = `url(${lazyImage.dataset.bg})`
          lazyImage.classList.remove("lazy")
          lazyImageObserver.unobserve(lazyImage)
        }
      });
    });

    lazyImages.forEach(function(lazyImage) {
      lazyImageObserver.observe(lazyImage)
    })
  } else {
    // Possibly fall back to a more compatible method here
  }
});

const menuContainer = document.querySelector('.menu-container')
const menus = document.querySelectorAll('.menu-container .menu')
const mask = document.querySelector('.mask')
const contentWrapper = document.querySelector('.content-wrapper')
const latestArticle = document.querySelector('.latest-article')
const readMore = document.querySelector('.read-more')
const indexPage = document.querySelector('.index-page')

const isHome = location.pathname === '/'
if (latestArticle) {
  latestArticle.style.display = isHome ? 'block' : 'none'
  readMore.style.display = isHome ? 'block' : 'none'
  indexPage.style.display = isHome ? 'none' : 'block'
}

const openMenu = () => {
  menuContainer.classList.add('open')
  menus.forEach(menu => {
    menu.classList.add('animated', 'fadeInLeft')
  })
  mask.classList.add('open')
  contentWrapper.classList.add('is-second')
}

const closeMenu = () => {
  menuContainer.classList.remove('open')
  menus.forEach(menu => {
    menu.classList.remove('animated', 'fadeInLeft')
  })
  mask.classList.remove('open')
  contentWrapper.classList.remove('is-second')
}
</script>
  
  </body>
</html>
