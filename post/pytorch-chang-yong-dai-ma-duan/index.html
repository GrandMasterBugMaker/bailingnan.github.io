<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>白凌南</title>
<meta name="description" content="DM/DL/RecSys/Python/XJTU/INTJ" />
<link rel="shortcut icon" href="https://bailingnan.github.io//favicon.ico?v=1587217732230">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.2/animate.min.css">

<link rel="stylesheet" href="https://bailingnan.github.io//styles/main.css">


<script async src="https://www.googletagmanager.com/gtag/js?id=UA-157390001-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-157390001-1');
</script>


  </head>
  <body>
    <div id="app" class="main px-4 flex flex-col lg:flex-row">
      <div id="sidebar" class="sidebar-wrapper lg:static lg:w-1/4">
  <div class="lg:sticky top-0">
    <div class="sidebar-content">
      <div class="flex lg:block p-4 lg:px-0 items-center fixed lg:static lg:block top-0 right-0 left-0 bg-white z-50">
        <i class="ri-menu-2-line lg:mt-4 text-2xl cursor-pointer animated fadeIn" onclick="openMenu()"></i>
        <a href="https://bailingnan.github.io/">
          <img class="animated fadeInLeft avatar rounded-lg mx-4 lg:mt-32 lg:mx-0 mt-0 lg:w-24 lg:h-24 w-12 w-12" src="https://bailingnan.github.io//images/avatar.png?v=1587217732230" alt="">
        </a>
        <h1 class="animated fadeInLeft lg:text-4xl font-extrabold lg:mt-8 mt-0 text-xl" style="animation-delay: 0.2s">白凌南</h1>
      </div>
      
        <div class="animated fadeInLeft" style="animation-delay: 0.4s">
          <p class="my-4 text-gray-600 font-light hidden lg:block">
            文章目录
          </p>
          <div class="toc-container hidden lg:block">
            <ul class="markdownIt-TOC">
<li><a href="#1%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE">1.基本配置</a>
<ul>
<li><a href="#%E5%AF%BC%E5%85%A5%E5%8C%85%E5%92%8C%E7%89%88%E6%9C%AC%E6%9F%A5%E8%AF%A2">导入包和版本查询</a></li>
<li><a href="#%E5%8F%AF%E5%A4%8D%E7%8E%B0%E6%80%A7">可复现性</a></li>
<li><a href="#%E6%98%BE%E5%8D%A1%E8%AE%BE%E7%BD%AE">显卡设置</a></li>
<li><a href="#%E8%AE%BE%E7%BD%AE%E4%B8%BAcudnn-benchmark%E6%A8%A1%E5%BC%8F">设置为cuDNN benchmark模式</a></li>
</ul>
</li>
<li><a href="#2%E5%BC%A0%E9%87%8Ftensor%E5%A4%84%E7%90%86">2.张量(Tensor)处理</a>
<ul>
<li><a href="#%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F">创建张量</a></li>
<li><a href="#%E5%BC%A0%E9%87%8F%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF">张量基本信息</a></li>
<li><a href="#%E5%88%87%E7%89%87%E4%B8%8E%E7%B4%A2%E5%BC%95">切片与索引</a></li>
<li><a href="#%E5%91%BD%E5%90%8D%E5%8F%98%E9%87%8F">命名变量</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2">数据类型转换</a></li>
<li><a href="#torchtensor%E4%B8%8Enpndarray%E8%BD%AC%E6%8D%A2">torch.Tensor与np.ndarray转换</a></li>
<li><a href="#%E4%BB%8E%E5%8F%AA%E5%8C%85%E5%90%AB%E4%B8%80%E4%B8%AA%E5%85%83%E7%B4%A0%E7%9A%84%E5%BC%A0%E9%87%8F%E4%B8%AD%E6%8F%90%E5%8F%96%E5%80%BC">从只包含一个元素的张量中提取值</a></li>
<li><a href="#%E5%BC%A0%E9%87%8F%E5%BD%A2%E5%8F%98">张量形变</a></li>
<li><a href="#%E5%BC%A0%E9%87%8F%E5%A4%8D%E5%88%B6">张量复制</a></li>
<li><a href="#%E5%BC%A0%E9%87%8F%E6%8B%BC%E6%8E%A5">张量拼接</a></li>
<li><a href="#%E5%B0%86%E6%95%B4%E6%95%B0%E6%A0%87%E7%AD%BE%E8%BD%AC%E4%B8%BAone-hot%E7%BC%96%E7%A0%81">将整数标签转为one-hot编码</a></li>
<li><a href="#%E5%BE%97%E5%88%B0%E9%9D%9E%E9%9B%B6%E5%85%83%E7%B4%A0">得到非零元素</a></li>
<li><a href="#%E5%88%A4%E6%96%AD%E4%B8%A4%E4%B8%AA%E5%BC%A0%E9%87%8F%E7%9B%B8%E7%AD%89">判断两个张量相等</a></li>
<li><a href="#%E5%BC%A0%E9%87%8F%E6%89%A9%E5%B1%95">张量扩展</a></li>
<li><a href="#%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95">矩阵乘法</a></li>
<li><a href="#%E8%AE%A1%E7%AE%97%E4%B8%A4%E7%BB%84%E6%95%B0%E6%8D%AE%E4%B9%8B%E9%97%B4%E7%9A%84%E4%B8%A4%E4%B8%A4%E6%AC%A7%E5%BC%8F%E8%B7%9D%E7%A6%BB">计算两组数据之间的两两欧式距离</a></li>
</ul>
</li>
<li><a href="#3%E6%A8%A1%E5%9E%8B%E5%AE%9A%E4%B9%89%E5%92%8C%E6%93%8D%E4%BD%9C">3.模型定义和操作</a>
<ul>
<li><a href="#%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B%E6%95%B4%E4%BD%93%E5%8F%82%E6%95%B0%E9%87%8F">计算模型整体参数量</a></li>
<li><a href="#%E6%9F%A5%E7%9C%8B%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%8F%82%E6%95%B0">查看网络中的参数</a></li>
<li><a href="#%E7%B1%BB%E4%BC%BC-keras-%E7%9A%84-modelsummary-%E8%BE%93%E5%87%BA%E6%A8%A1%E5%9E%8B%E4%BF%A1%E6%81%AF%E4%BD%BF%E7%94%A8pytorch-summary">类似 Keras 的 model.summary() 输出模型信息（使用pytorch-summary ）</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96">模型权重初始化</a></li>
<li><a href="#%E6%8F%90%E5%8F%96%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E6%9F%90%E4%B8%80%E5%B1%82">提取模型中的某一层</a></li>
<li><a href="#%E9%83%A8%E5%88%86%E5%B1%82%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B">部分层使用预训练模型</a></li>
</ul>
</li>
<li><a href="#4%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%92%8C%E6%B5%8B%E8%AF%95">4.模型训练和测试</a>
<ul>
<li><a href="#modeleval%E5%92%8Cwith-torchno_grad%E7%9A%84%E5%8C%BA%E5%88%AB"><code>model.eval()</code>和<code>with torch.no_grad()</code>的区别</a>
<ul>
<li><a href="#%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF">使用场景</a></li>
</ul>
</li>
<li><a href="#%E8%87%AA%E5%AE%9A%E4%B9%89loss">自定义loss</a></li>
<li><a href="#%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91label-smoothing">标签平滑（label smoothing）</a></li>
<li><a href="#l1-%E6%AD%A3%E5%88%99%E5%8C%96">L1 正则化</a></li>
<li><a href="#%E4%B8%8D%E5%AF%B9%E5%81%8F%E7%BD%AE%E9%A1%B9%E8%BF%9B%E8%A1%8C%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8Fweight-decay">不对偏置项进行权重衰减（weight decay）</a></li>
<li><a href="#%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AAgradient-clipping">梯度裁剪（gradient clipping）</a></li>
<li><a href="#%E5%BE%97%E5%88%B0%E5%BD%93%E5%89%8D%E5%AD%A6%E4%B9%A0%E7%8E%87">得到当前学习率</a></li>
<li><a href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F">学习率衰减</a></li>
<li><a href="#%E4%BC%98%E5%8C%96%E5%99%A8%E9%93%BE%E5%BC%8F%E6%9B%B4%E6%96%B0">优化器链式更新</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%A7%86%E5%8C%96">模型训练可视化</a></li>
<li><a href="#%E4%BF%9D%E5%AD%98%E4%B8%8E%E5%8A%A0%E8%BD%BD%E6%96%AD%E7%82%B9">保存与加载断点</a></li>
</ul>
</li>
<li><a href="#5%E5%85%B6%E4%BB%96%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9">5.其他注意事项</a>
<ul>
<li><a href="#%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96">数据读取</a></li>
<li><a href="#tricks">Tricks</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      
    </div>
  </div>
</div>

<div class="menu-container">
  <i class="ri-arrow-left-line text-2xl cursor-pointer animated fadeIn close-menu-btn" onclick="closeMenu()"></i>
  <div>
    
      
        <a href="/" class="menu" style="animation-delay: 0s">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu" style="animation-delay: 0.2s">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu" style="animation-delay: 0.4s">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu" style="animation-delay: 0.6000000000000001s">
          关于
        </a>
      
    
  </div>
  <div class="site-footer">
    <div class="py-4 text-gray-700"></div>
    <a class="rss" href="https://bailingnan.github.io//atom.xml" target="_blank">RSS</a>
  </div>
</div>
<div class="mask" onclick="closeMenu()">
</div>
      <div class="content-wrapper py-32 lg:p-8 lg:w-3/4 post-detail animated fadeIn">
        <h1 class="text-3xl font-bold lg:mt-16">PyTorch常用代码段</h1>
        <div class="text-sm text-gray-700 lg:my-8">
          2020-01-31 / 20 min read
        </div>
        
        <div class="post-content yue">
          <h1 id="1基本配置">1.基本配置</h1>
<h2 id="导入包和版本查询">导入包和版本查询</h2>
<pre><code class="language-Python">import torch
import torch.nn as nn
import torchvision
print(torch.__version__)# PyTorch version
print(torch.version.cuda)#Corresponding CUDA version
print(torch.backends.cudnn.version())#Corresponding cuDNN version
print(torch.cuda.get_device_name(0))#GPU type
</code></pre>
<h2 id="可复现性">可复现性</h2>
<p>在硬件设备（CPU、GPU）不同时，完全的可复现性无法保证，即使随机种子相同。但是，在同一个设备上，应该保证可复现性。具体做法是，在程序开始的时候固定torch的随机种子，同时也把numpy的随机种子固定。</p>
<pre><code class="language-Python">np.random.seed(0)
torch.manual_seed(0)
torch.cuda.manual_seed_all(0)
</code></pre>
<h2 id="显卡设置">显卡设置</h2>
<p>如果只需要一张显卡</p>
<pre><code class="language-Python"># Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
</code></pre>
<p>如果需要指定多张显卡，比如0，1号显卡。</p>
<pre><code class="language-Python">import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
</code></pre>
<p>也可以在命令行运行代码时设置显卡：</p>
<pre><code>CUDA_VISIBLE_DEVICES=0,1 python train.py
</code></pre>
<p>清除显存:</p>
<pre><code class="language-Python">torch.cuda.empty_cache()
</code></pre>
<p>也可以使用在命令行重置GPU的指令：</p>
<pre><code>nvidia-smi --gpu-reset -i [gpu_id]
</code></pre>
<p>或在命令行可以先使用ps找到程序的PID，再使用kill结束该进程</p>
<pre><code class="language-python">ps aux | grep python
kill -9 [pid]
</code></pre>
<h2 id="设置为cudnn-benchmark模式">设置为cuDNN benchmark模式</h2>
<p>Benchmark模式会提升计算速度，但是由于计算中有随机性，每次网络前馈结果略有差异。</p>
<pre><code class="language-python">torch.backends.cudnn.benchmark = True
</code></pre>
<p>如果想要避免这种结果波动，设置</p>
<pre><code class="language-python">torch.backends.cudnn.deterministic = True
</code></pre>
<h1 id="2张量tensor处理">2.张量(Tensor)处理</h1>
<h2 id="创建张量">创建张量</h2>
<pre><code class="language-python">#data只能是已知的数据，不能是数据的维度
torch.tensor(data)
#可以是data，也可以是维度
#data:
torch.Tensor(data)
#维度,维度为(1,2,3)
torch.Tensor(1,2,3)
</code></pre>
<h2 id="张量基本信息">张量基本信息</h2>
<pre><code class="language-Python">tensor = torch.randn(3,4,5)
print(tensor.type())  # 数据类型
print(tensor.size())  # 张量的shape，是个元组
print(tensor.dim())   # 维度的数量
</code></pre>
<h2 id="切片与索引">切片与索引</h2>
<p>将图片设定为[batch_size, channel, height, width]的四维矩阵。</p>
<pre><code class="language-python">a = torch.rand(4, 3, 28, 28)
</code></pre>
<p>对第一维进行索引,可以认为是第一个图片的三个维度通道的28*28的像素点。:</p>
<pre><code class="language-python">print(a[0].size())
torch.Size([3, 28, 28])
</code></pre>
<p>第一个图片的第一个维度通道的28*28的像素点:</p>
<pre><code class="language-python">print(a[0, 0].size())
torch.Size([28, 28])
</code></pre>
<p>具体到某一个像素点时:</p>
<pre><code class="language-python">print(a[0, 0, 2, 3])
tensor(0.4736)
</code></pre>
<p>取连续的索引:</p>
<pre><code class="language-python">print(a[:2].shape)
torch.Size([2, 3, 28, 28])
</code></pre>
<p>同理：</p>
<pre><code class="language-python"># 1写在：前面，表明从1个通道开始到末尾，,不包括1
print(a[:2, 1:, :, :].shape)
</code></pre>
<p>当索引出现-1时，要提到一个知识点:</p>
<pre><code class="language-python"># 默认索引的顺序为[0, 1, 2]，当倒着写时变为[-3, -2, -1]。由于这里取-1，因此为最后一位。
print(a[:2, -1:, :, :].shape)
torch.Size([2, 1, 28, 28])
</code></pre>
<p>当想隔点取样输出时:</p>
<pre><code class="language-python">#输出全部batch和channel，对每个高和宽间隔2个点采样
print(a[:, :, 0:28:2, 0:28:2].shape)
torch.Size([4, 3, 14, 14])
#可简化为：
print(a[:, :, ::2, ::2].shape)
</code></pre>
<h2 id="命名变量">命名变量</h2>
<pre><code class="language-Python"># 在PyTorch 1.3之前，需要使用注释
# Tensor[N, C, H, W]
images = torch.randn(32, 3, 56, 56)
images.sum(dim=1)
images.select(dim=1, index=0)

# PyTorch 1.3之后
NCHW = [‘N’, ‘C’, ‘H’, ‘W’]
images = torch.randn(32, 3, 56, 56, names=NCHW)
images.sum('C')
images.select('C', index=0)
# 也可以这么设置
tensor = torch.rand(3,4,1,2,names=('C', 'N', 'H', 'W'))
# 使用align_to可以对维度方便地排序
tensor = tensor.align_to('N', 'C', 'H', 'W')
</code></pre>
<h2 id="数据类型转换">数据类型转换</h2>
<pre><code class="language-Python"># 设置默认类型，pytorch中的FloatTensor远远快于DoubleTensor
torch.set_default_tensor_type(torch.FloatTensor)

# 类型转换
tensor = tensor.cuda()
tensor = tensor.cpu()
tensor = tensor.float()
tensor = tensor.long()
</code></pre>
<h2 id="torchtensor与npndarray转换">torch.Tensor与np.ndarray转换</h2>
<p>除了CharTensor，其他所有CPU上的张量都支持转换为numpy格式然后再转换回来。</p>
<pre><code class="language-Python">ndarray = tensor.cpu().numpy()
tensor = torch.from_numpy(ndarray).float()
tensor = torch.from_numpy(ndarray.copy()).float() # If ndarray has negative stride.
</code></pre>
<h2 id="从只包含一个元素的张量中提取值">从只包含一个元素的张量中提取值</h2>
<p><code>value = torch.rand(1).item()</code></p>
<h2 id="张量形变">张量形变</h2>
<pre><code class="language-Python"># 在将卷积层输入全连接层的情况下通常需要对张量做形变处理，
# 相比torch.view，torch.reshape可以自动处理输入张量不连续的情况。
tensor = torch.rand(2,3,4)
shape = (6, 4)
tensor = torch.reshape(tensor, shape)
</code></pre>
<h2 id="张量复制">张量复制</h2>
<pre><code class="language-Python"># Operation                 |  New/Shared memory | Still in computation graph |
tensor.clone()            # |        New         |          Yes               |
tensor.detach()           # |      Shared        |          No                |
tensor.detach.clone()()   # |        New         |          No                |
</code></pre>
<h2 id="张量拼接">张量拼接</h2>
<pre><code class="language-Python">'''
注意torch.cat和torch.stack的区别在于torch.cat沿着给定的维度拼接，
而torch.stack会新增一维。例如当参数是3个10x5的张量，torch.cat的结果是30x5的张量，
而torch.stack的结果是3x10x5的张量。
'''
tensor = torch.cat(list_of_tensors, dim=0)
tensor = torch.stack(list_of_tensors, dim=0)
</code></pre>
<h2 id="将整数标签转为one-hot编码">将整数标签转为one-hot编码</h2>
<pre><code class="language-python"># pytorch的标记默认从0开始
tensor = torch.tensor([0, 2, 1, 3])
N = tensor.size(0)
num_classes = 4
one_hot = torch.zeros(N, num_classes).long()
one_hot.scatter_(dim=1, index=torch.unsqueeze(tensor, dim=1), src=torch.ones(N, num_classes).long())
</code></pre>
<h2 id="得到非零元素">得到非零元素</h2>
<pre><code class="language-python">torch.nonzero(tensor)               # index of non-zero elements,包含点坐标的列表向量
torch.nonzero(tensor==0)            # index of zero elements
torch.nonzero(tensor).size(0)       # number of non-zero elements
torch.nonzero(tensor == 0).size(0)  # number of zero elements
</code></pre>
<h2 id="判断两个张量相等">判断两个张量相等</h2>
<pre><code class="language-Python">torch.allclose(tensor1, tensor2)  # float tensor
torch.equal(tensor1, tensor2)     # int tensor
</code></pre>
<h2 id="张量扩展">张量扩展</h2>
<pre><code class="language-python"># Expand tensor of shape 64*512 to shape 64*512*7*7.
tensor = torch.rand(64,512)
torch.reshape(tensor, (64, 512, 1, 1)).expand(64, 512, 7, 7)
</code></pre>
<h2 id="矩阵乘法">矩阵乘法</h2>
<pre><code class="language-python"># Matrix multiplcation: (m*n) * (n*p) * -&gt; (m*p).
result = torch.mm(tensor1, tensor2)

# Batch matrix multiplication: (b*m*n) * (b*n*p) -&gt; (b*m*p)
result = torch.bmm(tensor1, tensor2)

# Element-wise multiplication.
result = tensor1 * tensor2
</code></pre>
<h2 id="计算两组数据之间的两两欧式距离">计算两组数据之间的两两欧式距离</h2>
<p>利用broadcast机制</p>
<pre><code class="language-python">dist = torch.sqrt(torch.sum((X1[:,None,:] - X2) ** 2, dim=2))
</code></pre>
<h1 id="3模型定义和操作">3.模型定义和操作</h1>
<h2 id="计算模型整体参数量">计算模型整体参数量</h2>
<pre><code class="language-python">num_parameters = sum(torch.numel(parameter) for parameter in model.parameters())
</code></pre>
<h2 id="查看网络中的参数">查看网络中的参数</h2>
<p>可以通过model.state_dict()或者model.named_parameters()函数查看现在的全部可训练参数（包括通过继承得到的父类中的参数）</p>
<pre><code class="language-python">params = list(model.named_parameters())
(name, param) = params[28]
print(name)
print(param.grad)
print('-------------------------------------------------')
(name2, param2) = params[29]
print(name2)
print(param2.grad)
print('----------------------------------------------------')
(name1, param1) = params[30]
print(name1)
print(param1.grad)
</code></pre>
<h2 id="类似-keras-的-modelsummary-输出模型信息使用pytorch-summary">类似 Keras 的 model.summary() 输出模型信息（使用pytorch-summary ）</h2>
<h2 id="模型权重初始化">模型权重初始化</h2>
<p>注意 model.modules() 和 model.children() 的区别：model.modules() 会迭代地遍历模型的所有子层，而 model.children() 只会遍历模型下的一层。</p>
<pre><code class="language-python"># Common practise for initialization.
for layer in model.modules():
    if isinstance(layer, torch.nn.Conv2d):
        torch.nn.init.kaiming_normal_(layer.weight, mode='fan_out',
                                      nonlinearity='relu')
        if layer.bias is not None:
            torch.nn.init.constant_(layer.bias, val=0.0)
    elif isinstance(layer, torch.nn.BatchNorm2d):
        torch.nn.init.constant_(layer.weight, val=1.0)
        torch.nn.init.constant_(layer.bias, val=0.0)
    elif isinstance(layer, torch.nn.Linear):
        torch.nn.init.xavier_normal_(layer.weight)
        if layer.bias is not None:
            torch.nn.init.constant_(layer.bias, val=0.0)

# Initialization with given tensor.
layer.weight = torch.nn.Parameter(tensor)
</code></pre>
<h2 id="提取模型中的某一层">提取模型中的某一层</h2>
<p>modules()会返回模型中所有模块的迭代器，它能够访问到最内层，比如self.layer1.conv1这个模块，还有一个与它们相对应的是name_children()属性以及named_modules(),这两个不仅会返回模块的迭代器，还会返回网络层的名字。</p>
<pre><code class="language-python"># 取模型中的前两层
new_model = nn.Sequential(*list(model.children())[:2] 
# 如果希望提取出模型中的所有卷积层，可以像下面这样操作：
for layer in model.named_modules():
    if isinstance(layer[1],nn.Conv2d):
         conv_model.add_module(layer[0],layer[1])
</code></pre>
<h2 id="部分层使用预训练模型">部分层使用预训练模型</h2>
<p>注意如果保存的模型是 torch.nn.DataParallel，则当前的模型也需要是</p>
<pre><code class="language-python">model.load_state_dict(torch.load('model.pth'), strict=False)
</code></pre>
<h1 id="4模型训练和测试">4.模型训练和测试</h1>
<h2 id="modeleval和with-torchno_grad的区别"><code>model.eval()</code>和<code>with torch.no_grad()</code>的区别</h2>
<p>在PyTorch中进行validation时，会使用<code>model.eval()</code>切换到测试模式，在该模式下，</p>
<ul>
<li>主要用于通知dropout层和batchnorm层在train和val模式间切换<br>
在train模式下，dropout网络层会按照设定的参数p设置保留激活单元的概率（保留概率=p); batchnorm层会继续计算数据的mean和var等参数并更新。<br>
在val模式下，dropout层会让所有的激活单元都通过，而batchnorm层会停止计算和更新mean和var，直接使用在训练阶段已经学出的mean和var值。</li>
<li>该模式不会影响各层的gradient计算行为，即gradient计算和存储与training模式一样，只是不进行反传（backprobagation）<br>
而<code>with torch.zero_grad()</code>则主要是用于停止autograd模块的工作，以起到加速和节省显存的作用，具体行为就是停止gradient计算，从而节省了GPU算力和显存，但是并不会影响dropout和batchnorm层的行为。</li>
</ul>
<h3 id="使用场景">使用场景</h3>
<p>如果不在意显存大小和计算时间的话，仅仅使用<code>model.eval()</code>已足够得到正确的validation的结果；而<code>with torch.zero_grad()</code>则是更进一步加速和节省gpu空间（因为不用计算和存储gradient），从而可以更快计算，也可以跑更大的batch来测试。</p>
<h2 id="自定义loss">自定义loss</h2>
<p>继承torch.nn.Module类写自己的loss。</p>
<pre><code class="language-python">class MyLoss(torch.nn.Moudle):
    def __init__(self):
        super(MyLoss, self).__init__()
        
    def forward(self, x, y):
        loss = torch.mean((x - y) ** 2)
        return loss
</code></pre>
<h2 id="标签平滑label-smoothing">标签平滑（label smoothing）</h2>
<p>写一个label_smoothing.py的文件，然后在训练代码里引用，用LSR代替交叉熵损失即可。label_smoothing.py内容如下：</p>
<pre><code class="language-python">import torch
import torch.nn as nn


class LSR(nn.Module):

    def __init__(self, e=0.1, reduction='mean'):
        super().__init__()

        self.log_softmax = nn.LogSoftmax(dim=1)
        self.e = e
        self.reduction = reduction
    
    def _one_hot(self, labels, classes, value=1):
        &quot;&quot;&quot;
            Convert labels to one hot vectors
        
        Args:
            labels: torch tensor in format [label1, label2, label3, ...]
            classes: int, number of classes
            value: label value in one hot vector, default to 1
        
        Returns:
            return one hot format labels in shape [batchsize, classes]
        &quot;&quot;&quot;

        one_hot = torch.zeros(labels.size(0), classes)

        #labels and value_added  size must match
        labels = labels.view(labels.size(0), -1)
        value_added = torch.Tensor(labels.size(0), 1).fill_(value)

        value_added = value_added.to(labels.device)
        one_hot = one_hot.to(labels.device)

        one_hot.scatter_add_(1, labels, value_added)

        return one_hot

    def _smooth_label(self, target, length, smooth_factor):
        &quot;&quot;&quot;convert targets to one-hot format, and smooth
        them.
        Args:
            target: target in form with [label1, label2, label_batchsize]
            length: length of one-hot format(number of classes)
            smooth_factor: smooth factor for label smooth
        
        Returns:
            smoothed labels in one hot format
        &quot;&quot;&quot;
        one_hot = self._one_hot(target, length, value=1 - smooth_factor)
        one_hot += smooth_factor / (length - 1)

        return one_hot.to(target.device)

    def forward(self, x, target):

        if x.size(0) != target.size(0):
            raise ValueError('Expected input batchsize ({}) to match target batch_size({})'
                    .format(x.size(0), target.size(0)))

        if x.dim() &lt; 2:
            raise ValueError('Expected input tensor to have least 2 dimensions(got {})'
                    .format(x.size(0)))

        if x.dim() != 2:
            raise ValueError('Only 2 dimension tensor are implemented, (got {})'
                    .format(x.size()))


        smoothed_target = self._smooth_label(target, x.size(1), self.e)
        x = self.log_softmax(x)
        loss = torch.sum(- x * smoothed_target, dim=1)

        if self.reduction == 'none':
            return loss
        
        elif self.reduction == 'sum':
            return torch.sum(loss)
        
        elif self.reduction == 'mean':
            return torch.mean(loss)
        
        else:
            raise ValueError('unrecognized option, expect reduction to be one of none, mean, sum')
</code></pre>
<p>或者直接在训练文件里做label smoothing</p>
<pre><code class="language-python">for images, labels in train_loader:
    images, labels = images.cuda(), labels.cuda()
    N = labels.size(0)
    # C is the number of classes.
    smoothed_labels = torch.full(size=(N, C), fill_value=0.1 / (C - 1)).cuda()
    smoothed_labels.scatter_(dim=1, index=torch.unsqueeze(labels, dim=1), value=0.9)

    score = model(images)
    log_prob = torch.nn.functional.log_softmax(score, dim=1)
    loss = -torch.sum(log_prob * smoothed_labels) / N
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()    
</code></pre>
<h2 id="l1-正则化">L1 正则化</h2>
<pre><code class="language-python">l1_regularization = torch.nn.L1Loss(reduction='sum')
loss = ...  # Standard cross-entropy loss
for param in model.parameters():
    loss += torch.sum(torch.abs(param))
loss.backward()
</code></pre>
<h2 id="不对偏置项进行权重衰减weight-decay">不对偏置项进行权重衰减（weight decay）</h2>
<p>pytorch里的weight decay相当于l2正则</p>
<pre><code class="language-python">bias_list = (param for name, param in model.named_parameters() if name[-4:] == 'bias')
others_list = (param for name, param in model.named_parameters() if name[-4:] != 'bias')
parameters = [{'parameters': bias_list, 'weight_decay': 0},                
              {'parameters': others_list}]
optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4)
</code></pre>
<h2 id="梯度裁剪gradient-clipping">梯度裁剪（gradient clipping）</h2>
<pre><code class="language-python">torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20)
</code></pre>
<h2 id="得到当前学习率">得到当前学习率</h2>
<pre><code class="language-python"># If there is one global learning rate (which is the common case).
lr = next(iter(optimizer.param_groups))['lr']

# If there are multiple learning rates for different layers.
all_lr = []
for param_group in optimizer.param_groups:
    all_lr.append(param_group['lr'])
</code></pre>
<p>另一种方法，在一个batch训练代码里，当前的lr是</p>
<pre><code class="language-python">optimizer.param_groups[0]['lr']
</code></pre>
<h2 id="学习率衰减">学习率衰减</h2>
<pre><code class="language-python"># Reduce learning rate when validation accuarcy plateau.
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, verbose=True)
for t in range(0, 80):
    train(...)
    val(...)
    scheduler.step(val_acc)

# Cosine annealing learning rate.
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=80)
# Reduce learning rate by 10 at given epochs.
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 70], gamma=0.1)
for t in range(0, 80):
    scheduler.step()    
    train(...)
    val(...)

# Learning rate warmup by 10 epochs.
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: t / 10)
for t in range(0, 10):
    scheduler.step()
    train(...)
    val(...)
</code></pre>
<h2 id="优化器链式更新">优化器链式更新</h2>
<p>从1.4版本开始，<code>torch.optim.lr_scheduler</code> 支持链式更新（chaining），即用户可以定义两个 schedulers，并交替在训练中使用。</p>
<pre><code class="language-python">import torch
from torch.optim import SGD
from torch.optim.lr_scheduler import ExponentialLR, StepLR
model = [torch.nn.Parameter(torch.randn(2, 2, requires_grad=True))]
optimizer = SGD(model, 0.1)
scheduler1 = ExponentialLR(optimizer, gamma=0.9)
scheduler2 = StepLR(optimizer, step_size=3, gamma=0.1)
for epoch in range(4):
    print(epoch, scheduler2.get_last_lr()[0])
    optimizer.step()
    scheduler1.step()
    scheduler2.step()
</code></pre>
<h2 id="模型训练可视化">模型训练可视化</h2>
<p>PyTorch可以使用tensorboard来可视化训练过程。<br>
安装和运行TensorBoard。</p>
<pre><code class="language-python">pip install tensorboard
tensorboard --logdir=runs
</code></pre>
<p>使用SummaryWriter类来收集和可视化相应的数据，放了方便查看，可以使用不同的文件夹，比如'Loss/train'和'Loss/test'。</p>
<pre><code class="language-python">from torch.utils.tensorboard import SummaryWriter
import numpy as np

writer = SummaryWriter()

for n_iter in range(100):
    writer.add_scalar('Loss/train', np.random.random(), n_iter)
    writer.add_scalar('Loss/test', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)
</code></pre>
<h2 id="保存与加载断点">保存与加载断点</h2>
<p>注意为了能够恢复训练，我们需要同时保存模型和优化器的状态，以及当前的训练轮数。</p>
<pre><code class="language-python">start_epoch = 0
# Load checkpoint.
if resume: # resume为参数，第一次训练时设为0，中断再训练时设为1
    model_path = os.path.join('model', 'best_checkpoint.pth.tar')
    assert os.path.isfile(model_path)
    checkpoint = torch.load(model_path)
    best_acc = checkpoint['best_acc']
    start_epoch = checkpoint['epoch']
    model.load_state_dict(checkpoint['model'])
    optimizer.load_state_dict(checkpoint['optimizer'])
    print('Load checkpoint at epoch {}.'.format(start_epoch))
    print('Best accuracy so far {}.'.format(best_acc))

# Train the model
for epoch in range(start_epoch, num_epochs): 
    ... 

    # Test the model
    ...
        
    # save checkpoint
    is_best = current_acc &gt; best_acc
    best_acc = max(current_acc, best_acc)
    checkpoint = {
        'best_acc': best_acc,
        'epoch': epoch + 1,
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict(),
    }
    model_path = os.path.join('model', 'checkpoint.pth.tar')
    best_model_path = os.path.join('model', 'best_checkpoint.pth.tar')
    torch.save(checkpoint, model_path)
    if is_best:
        shutil.copy(model_path, best_model_path)
</code></pre>
<h1 id="5其他注意事项">5.其他注意事项</h1>
<h2 id="数据读取">数据读取</h2>
<p>使用<code>read_csv</code>读取一次原始文件，将<code>dataframe</code>存储为<code>HDF</code>或者<code>feather</code>格式。一般情况下<code>HDF</code>的读取比读取<code>csv</code>文件快几十倍，但<code>HDF</code>文件在大小上会稍微大一些。</p>
<h2 id="tricks">Tricks</h2>
<ol>
<li>建议有参数的层和汇合（pooling）层使用<code>torch.nn</code>模块定义，激活函数直接使用<code>torch.nn.functional。torch.nn</code>模块和<code>torch.nn.functional</code>的区别在于，<code>torch.nn</code>模块在计算时底层调用了<code>torch.nn.functional</code>，但<code>torch.nn</code>模块包括该层参数，还可以应对训练和测试两种网络状态。使用<code>torch.nn.functional</code>时要注意网络状态，如</li>
</ol>
<pre><code class="language-python">def forward(self, x):
  ...
  x = torch.nn.functional.dropout(x, p=0.5, training=self.training)
</code></pre>
<ol start="2">
<li>不要使用太大的线性层。因为nn.Linear(m,n)使用的是O(mn)的内存，线性层太大很容易超出现有显存。</li>
<li>不要在太长的序列上使用RNN。因为RNN反向传播使用的是BPTT算法，其需要的内存和输入序列的长度呈线性关系。</li>
<li>model(x) 前用 <code>model.train()</code> 和 <code>model.eval()</code> 切换网络状态。</li>
<li>不需要计算梯度的代码块用 with torch.no_grad() 包含起来。</li>
<li><code>model.eval()</code> 和 <code>torch.no_grad()</code> 的区别在于，<code>model.eval()</code> 是将网络切换为测试状态，例如 BN 和dropout在训练和测试阶段使用不同的计算方法。<code>torch.no_grad()</code>是关闭 PyTorch 张量的自动求导机制，以减少存储使用和加速计算，得到的结果无法进行 <code>loss.backward()</code>。</li>
<li><code>model.zero_grad()</code>会把整个模型的参数的梯度都归零, 而<code>optimizer.zero_grad()</code>只会把传入其中的参数的梯度归零。</li>
<li>torch.nn.CrossEntropyLoss 的输入不需要经过 Softmax。torch.nn.CrossEntropyLoss 等价于 torch.nn.functional.log_softmax + torch.nn.NLLLoss。</li>
<li><code>loss.backward()</code>前用 <code>optimizer.zero_grad()</code> 清除累积梯度。</li>
<li><code>torch.utils.data.DataLoader</code> 中尽量设置 <code>pin_memory=True</code>，对特别小的数据集如 MNIST 设置 <code>pin_memory=False</code> 反而更快一些。<code>num_workers</code> 的设置需要在实验中找到最快的取值。</li>
<li>用 <code>del</code>及时删除不用的中间变量，节约 GPU 存储。</li>
<li>使用 <code>inplace</code> 操作可节约 GPU 存储，如</li>
</ol>
<pre><code class="language-python">x = torch.nn.functional.relu(x, inplace=True)
</code></pre>
<ol start="13">
<li>减少 CPU 和 GPU 之间的数据传输。例如如果你想知道一个 epoch 中每个 mini-batch 的 loss 和准确率，先将它们累积在 GPU 中等一个 epoch 结束之后一起传输回 CPU 会比每个 mini-batch 都进行一次 GPU 到 CPU 的传输更快。</li>
<li>使用半精度浮点数 <code>half()</code>会有一定的速度提升，具体效率依赖于 GPU 型号。需要小心数值精度过低带来的稳定性问题。</li>
<li>时常使用 <code>assert tensor.size() == (N, D, H, W)</code> 作为调试手段，确保张量维度和你设想中一致。</li>
<li>除了标记 y 外，尽量少使用一维张量，使用 n*1 的二维张量代替，可以避免一些意想不到的一维张量计算结果。</li>
<li>统计代码各部分耗时</li>
</ol>
<pre><code class="language-python">with torch.autograd.profiler.profile(enabled=True, use_cuda=False) as profile:
    ...
print(profile)

# 或者在命令行运行
python -m torch.utils.bottleneck main.py
</code></pre>
<ol start="18">
<li>使用TorchSnooper来调试PyTorch代码，程序在执行的时候，就会自动 print 出来每一行的执行结果的 tensor 的形状、数据类型、设备、是否需要梯度的信息。</li>
</ol>
<pre><code class="language-python"># pip install torchsnooper
import torchsnooper

# 对于函数，使用修饰器
@torchsnooper.snoop()

# 如果不是函数，使用 with 语句来激活 TorchSnooper，把训练的那个循环装进 with 语句中去。
with torchsnooper.snoop():
    原本的代码
</code></pre>
<ol start="19">
<li>训练过程记录及可视化:</li>
</ol>
<ul>
<li><code>wandb</code></li>
<li><code>comet_ml</code></li>
<li><code>mlflow</code></li>
</ul>
<ol start="20">
<li>一些扩展包：</li>
</ol>
<ul>
<li><code>torch-optimizer</code>:实现了最新的一些优化器.</li>
<li><code>fastai</code>:有一些评价指标</li>
<li><code>numba</code>:<code>import numba as nb</code>,纯<code>python</code>或<code>numpy</code>函数加装饰器,加速计算，加@nb.njit或@nb.jit(nopython=True)</li>
<li><code>swifter</code>:<code>df.apply()</code>→<code>df.swifter.apply()</code>，加速<code>pandas</code></li>
<li><code>captum</code>:可解释性</li>
<li><code>cupy</code>:加速<code>pandas</code>,1000万以上数据更快</li>
<li><code>modin</code>:<code>import modin.pandas as mdpd</code>,用<code>mdpd</code>代替<code>pd</code>即可，加速<code>pandas</code>,加载数据和查询数据更快,统计方法<code>pandas</code>更快</li>
</ul>

        </div>

        
          <a class="animated fadeInUp p-2 items-center text-sm text-gray-700 border hover:bg-gray-300 leading-none rounded-full flex lg:inline-flex m-4 " href="https://bailingnan.github.io/tag/kXu4iR4pX/">
            <span class="flex-auto">DL</span>
          </a>
        
          <a class="animated fadeInUp p-2 items-center text-sm text-gray-700 border hover:bg-gray-300 leading-none rounded-full flex lg:inline-flex m-4 " href="https://bailingnan.github.io/tag/0bvinX4U3/">
            <span class="flex-auto">PyTorch</span>
          </a>
        


        <div class="flex justify-between py-8">
          
            <div class="prev-post">
              <a href="https://bailingnan.github.io/post/pycharm-chang-yong-kuai-jie-jian-ji-ji-qiao-macos/">
                <h3 class="post-title">
                  <i class="ri-arrow-left-line"></i>
                  Pycharm常用快捷键及技巧(macOS)
                </h3>
              </a>
            </div>
          

          
            <div class="next-post">
              <a href="https://bailingnan.github.io/post/linux-chang-yong-ming-ling/">
                <h3 class="post-title">
                  Linux笔记
                  <i class="ri-arrow-right-line"></i>
                </h3>
              </a>
            </div>
          
        </div>

        
          
            <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<div id="gitalk-container"></div>

<script>

  var gitalk = new Gitalk({
    clientID: 'f4bd8cbd743b58235644',
    clientSecret: 'df6eff6e7836378726d917c527c47e86f1c30f8b',
    repo: 'bailingnan.github.io',
    owner: 'bailingnan',
    admin: ['bailingnan'],
    id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
  })

  gitalk.render('gitalk-container')

</script>

          

          
        

      </div>
    </div>

    <script src="https://bailingnan.github.io//media/prism.js"></script>  
<script>

Prism.highlightAll()
let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

// This should probably be throttled.
// Especially because it triggers during smooth scrolling.
// https://lodash.com/docs/4.17.10#throttle
// You could do like...
// window.addEventListener("scroll", () => {
//    _.throttle(doThatStuff, 100);
// });
// Only not doing it here to keep this Pen dependency-free.

window.addEventListener("scroll", event => {
  let fromTop = window.scrollY;

  mainNavLinks.forEach((link, index) => {
    let section = document.getElementById(decodeURI(link.hash).substring(1));
    let nextSection = null
    if (mainNavLinks[index + 1]) {
      nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
    }
    if (section.offsetTop <= fromTop) {
      if (nextSection) {
        if (nextSection.offsetTop > fromTop) {
          link.classList.add("current");
        } else {
          link.classList.remove("current");    
        }
      } else {
        link.classList.add("current");
      }
    } else {
      link.classList.remove("current");
    }
  });
});


document.addEventListener("DOMContentLoaded", function() {
  var lazyImages = [].slice.call(document.querySelectorAll(".post-feature-image.lazy"));

  if ("IntersectionObserver" in window) {
    let lazyImageObserver = new IntersectionObserver(function(entries, observer) {
      entries.forEach(function(entry) {
        if (entry.isIntersecting) {
          let lazyImage = entry.target
          lazyImage.style.backgroundImage = `url(${lazyImage.dataset.bg})`
          lazyImage.classList.remove("lazy")
          lazyImageObserver.unobserve(lazyImage)
        }
      });
    });

    lazyImages.forEach(function(lazyImage) {
      lazyImageObserver.observe(lazyImage)
    })
  } else {
    // Possibly fall back to a more compatible method here
  }
});

const menuContainer = document.querySelector('.menu-container')
const menus = document.querySelectorAll('.menu-container .menu')
const mask = document.querySelector('.mask')
const contentWrapper = document.querySelector('.content-wrapper')
const latestArticle = document.querySelector('.latest-article')
const readMore = document.querySelector('.read-more')
const indexPage = document.querySelector('.index-page')

const isHome = location.pathname === '/'
if (latestArticle) {
  latestArticle.style.display = isHome ? 'block' : 'none'
  readMore.style.display = isHome ? 'block' : 'none'
  indexPage.style.display = isHome ? 'none' : 'block'
}

const openMenu = () => {
  menuContainer.classList.add('open')
  menus.forEach(menu => {
    menu.classList.add('animated', 'fadeInLeft')
  })
  mask.classList.add('open')
  contentWrapper.classList.add('is-second')
}

const closeMenu = () => {
  menuContainer.classList.remove('open')
  menus.forEach(menu => {
    menu.classList.remove('animated', 'fadeInLeft')
  })
  mask.classList.remove('open')
  contentWrapper.classList.remove('is-second')
}
</script>
  
  </body>
</html>
