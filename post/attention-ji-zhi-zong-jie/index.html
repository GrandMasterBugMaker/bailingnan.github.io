<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>白凌南</title>
<meta name="description" content="DL/RecSys/Python/Java/INTJ" />
<link rel="shortcut icon" href="https://bailingnan.github.io//favicon.ico?v=1577035953242">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link href="https://cdn.remixicon.com/releases/v1.3.1/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.2/animate.min.css">

<link rel="stylesheet" href="https://bailingnan.github.io//styles/main.css">
<link rel="alternate" type="application/atom+xml" title="白凌南 - Atom Feed" href="https://bailingnan.github.io//atom.xml">



  </head>
  <body>
    <div id="app" class="main px-4 flex flex-col lg:flex-row">
      <div id="sidebar" class="sidebar-wrapper lg:static lg:w-1/4">
  <div class="lg:sticky top-0">
    <div class="sidebar-content">
      <div class="flex lg:block p-4 lg:px-0 items-center fixed lg:static lg:block top-0 right-0 left-0 bg-white z-50">
        <i class="remixicon-menu-2-line lg:mt-4 text-2xl cursor-pointer animated fadeIn" onclick="openMenu()"></i>
        <a href="https://bailingnan.github.io/">
          <img class="animated fadeInLeft avatar rounded-lg mx-4 lg:mt-32 lg:mx-0 mt-0 lg:w-24 lg:h-24 w-12 w-12" src="https://bailingnan.github.io//images/avatar.png?v=1577035953242" alt="">
        </a>
        <h1 class="animated fadeInLeft lg:text-4xl font-extrabold lg:mt-8 mt-0 text-xl" style="animation-delay: 0.2s">白凌南</h1>
      </div>
      
        <div class="animated fadeInLeft" style="animation-delay: 0.4s">
          <p class="my-4 text-gray-600 font-light hidden lg:block">
            文章目录
          </p>
          <div class="toc-container hidden lg:block">
            <ul class="markdownIt-TOC">
<li><a href="#attn_model-general">attn_model = 'general'</a></li>
<li><a href="#attn_model-concat">attn_model = 'concat'</a></li>
<li><a href="#luong-attention-layer">Luong attention layer</a></li>
</ul>

          </div>
        </div>
      
    </div>
  </div>
</div>

<div class="menu-container">
  <i class="remixicon-arrow-left-line text-2xl cursor-pointer animated fadeIn close-menu-btn" onclick="closeMenu()"></i>
  <div>
    
      
        <a href="/" class="menu" style="animation-delay: 0s">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu" style="animation-delay: 0.2s">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu" style="animation-delay: 0.4s">
          标签
        </a>
      
    
      
        <a href="/post/about" class="menu" style="animation-delay: 0.6000000000000001s">
          关于
        </a>
      
    
  </div>
  <div class="site-footer">
    <div class="py-4 text-gray-700"></div>
    <a class="rss" href="https://bailingnan.github.io//atom.xml" target="_blank">RSS</a>
  </div>
</div>
<div class="mask" onclick="closeMenu()">
</div>
      <div class="content-wrapper py-32 lg:p-8 lg:w-3/4 post-detail animated fadeIn">
        <h1 class="text-3xl font-bold lg:mt-16">Attention机制总结</h1>
        <div class="text-sm text-gray-700 lg:my-8">
          2019-12-23 / 11 min read
        </div>
        
        <div class="post-content yue">
          <p>Attention</p>
<p>在哪里体现了Attention呢？答案是: c_{i}，这个 Context Vector，上下文向量。<br>
区别于 Seq2Seq 直接把最后一个时序 i 的输出 h_{i} 作为上下文向量，而是将之前所有时序的输出通过加权求和得到的一个上下文向量。这个 c_{i} 包含着各个时序输出的权重信息，也就相当于告诉我们哪一段文字对于当前的 target word 是重要的，哪些是不重要的。这就相当于告诉我们的注意力应该放在哪里。<br>
c_{i} 是 Attention 矩阵的一行，表示输入 x_{1} 到 x_{t} 分别对 decoder 第 i 时序这个 target word 所对应的的权重（注意力大小）。<br>
\alpha_{ij} 是 Attention 矩阵的一个值，表示输入表示 x_{j} 对 decoder 第 i 时序的 target word 的权重。<br>
Attention 矩阵的计算方法如红框所示，由一个线性层和 softmax 层叠加得到。其输入是上一时刻 decoder 的状态 s_{i-1} 和 encoder 对第 j 个词的输出 h_{j} ， W_{a} 、 U_{a} 和 v_{a} 则是需要我们模型去学习的参数。其对应的物理意义就是：句子中某个词对应正准备翻译的词的权重（重要性），由词语本身（ h_{j} ）和前一个翻译的词（ s_{i-1} ）决定。<br>
Pytorch Tutorial神经网络翻译代码分析</p>
<p>使用前一步解码器的状态，先初始化：<br>
encoder_hidden = encoder.initHidden()<br>
decoder_hidden = encoder_hidden<br>
先进行attention的计算，再进入GRU，是Bahdanau等人的方式，有别于Luong Attention：<br>
class AttnDecoderRNN(nn.Module):<br>
def <strong>init</strong>(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):<br>
super(AttnDecoderRNN, self).<strong>init</strong>()<br>
self.hidden_size = hidden_size<br>
self.output_size = output_size<br>
self.dropout_p = dropout_p<br>
self.max_length = max_length</p>
<pre><code>    self.embedding = nn.Embedding(self.output_size, self.hidden_size)
    self.attn = nn.Linear(self.hidden_size * 2, self.max_length)
    self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)
    self.dropout = nn.Dropout(self.dropout_p)
    self.gru = nn.GRU(self.hidden_size, self.hidden_size)
    self.out = nn.Linear(self.hidden_size, self.output_size)

def forward(self, input, hidden, encoder_outputs):
    embedded = self.embedding(input).view(1, 1, -1)
    embedded = self.dropout(embedded)
    #embedded维度=[max_len(1,一个单词),batchsize(1,一个单词),hiddensize)
    #hidden维度=[num_layers(1),batchsize(1),hiddensize)
    attn_weights = F.softmax(
        self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)#[1,10]
    #encoder_outputs维度=[max_len,hiddensize]
    attn_applied = torch.bmm(attn_weights.unsqueeze(0),
                             encoder_outputs.unsqueeze(0))

    output = torch.cat((embedded[0], attn_applied[0]), 1)
    output = self.attn_combine(output).unsqueeze(0)

    output = F.relu(output)
    output, hidden = self.gru(output, hidden)

    output = F.log_softmax(self.out(output[0]), dim=1)
    return output, hidden, attn_weights

def initHidden(self):
    return torch.zeros(1, 1, self.hidden_size, device=device)
</code></pre>
<p>Pytorch Tutorial聊天机器人代码分析<br>
调用代码：<br>
model_name = 'cb_model'<br>
attn_model = 'dot'</p>
<h1 id="attn_model-general">attn_model = 'general'</h1>
<h1 id="attn_model-concat">attn_model = 'concat'</h1>
<p>hidden_size = 500<br>
encoder_n_layers = 2<br>
decoder_n_layers = 2<br>
dropout = 0.1<br>
batch_size = 64<br>
encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)<br>
decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)<br>
Encoder部分：<br>
输入:<br>
input_seq：一批输入句子; shape =（max_length，batch_size）<br>
input_lengths：一批次中每个句子对应的句子长度列表;shape=(batch_size)<br>
hidden:隐藏状态; shape =(n_layers x num_directions，batch_size，hidden_size)<br>
输出：<br>
outputs：GRU最后一个隐藏层的输出特征（双向输出之和）; shape =（max_length，batch_size，hidden_size）<br>
hidden：从GRU更新隐藏状态; shape =（n_layers x num_directions，batch_size，hidden_size）<br>
代码：<br>
class EncoderRNN(nn.Module):<br>
def <strong>init</strong>(self, hidden_size, embedding, n_layers=1, dropout=0):<br>
super(EncoderRNN, self).<strong>init</strong>()<br>
self.n_layers = n_layers<br>
self.hidden_size = hidden_size<br>
self.embedding = embedding</p>
<pre><code>    # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'
    #   because our input size is a word embedding with number of features == hidden_size
    self.gru = nn.GRU(hidden_size, hidden_size, n_layers,
                      dropout=(0 if n_layers == 1 else dropout), bidirectional=True)

def forward(self, input_seq, input_lengths, hidden=None):
    # Convert word indexes to embeddings
    embedded = self.embedding(input_seq)
    # Pack padded batch of sequences for RNN module
    packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)
    # Forward pass through GRU
    outputs, hidden = self.gru(packed, hidden)
    # Unpack padding
    outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)
    # Sum bidirectional GRU outputs
    outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]
    # Return output and final hidden state
    #hidden的维度=[num_layers*2,batchsize,hiddensize]
    #outputs的维度=[max_len,batchsize,hiddensize]
    return outputs, hidden
</code></pre>
<p>Decoder部分：<br>
通过“Global attention”，我们仅使用当前步的解码器的隐藏状态来计算注意力权重（或者能量）。 Bahdanau等人的注意力计算需要知道前一步中解码器的状态。 此外，Luong等人提供各种方法来计算编码器输出和解码器输出之间的注意权重（能量），称之为“score functions”：</p>
<p>其中 h_{t} = 当前目标解码器状态， \bar{h}_{s} = 所有编码器状态。<br>
计算注意力权重：用的是所有编码器状态encoder_outputs和经过GRU后的outputs<br>
以general为例：（其实不是很能理解这种权重计算方式</p>
<h1 id="luong-attention-layer">Luong attention layer</h1>
<p>class Attn(torch.nn.Module):<br>
def <strong>init</strong>(self, method, hidden_size):<br>
super(Attn, self).<strong>init</strong>()<br>
self.method = method<br>
if self.method not in ['dot', 'general', 'concat']:<br>
raise ValueError(self.method, &quot;is not an appropriate attention method.&quot;)<br>
self.hidden_size = hidden_size<br>
if self.method == 'general':<br>
self.attn = torch.nn.Linear(self.hidden_size, hidden_size)<br>
elif self.method == 'concat':<br>
self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)<br>
self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))</p>
<pre><code>def dot_score(self, hidden, encoder_output):
    return torch.sum(hidden * encoder_output, dim=2)

def general_score(self, hidden, encoder_output):
    #hidden维度=[num_layers,batchsize,hiddensize]
    energy = self.attn(encoder_output)
    #energy维度=[max_len,batchsize,hiddensize]
    return torch.sum(hidden * energy, dim=2)

def concat_score(self, hidden, encoder_output):
    energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()
    return torch.sum(self.v * energy, dim=2)

def forward(self, hidden, encoder_outputs):
    #hidden维度=[1,batchsize,hiddensize],步长为１
    #output维度=[max_len,batchsize,hiddensize]
    # Calculate the attention weights (energies) based on the given method
    if self.method == 'general':
        attn_energies = self.general_score(hidden, encoder_outputs)
    #维度=[max_len,batchsize]
    elif self.method == 'concat':
        attn_energies = self.concat_score(hidden, encoder_outputs)
    elif self.method == 'dot':
        attn_energies = self.dot_score(hidden, encoder_outputs)

    # Transpose max_length and batch_size dimensions
    attn_energies = attn_energies.t()#[batchsize,max_len]

    # Return the softmax normalized probability scores (with added dimension)
    return F.softmax(attn_energies, dim=1).unsqueeze(1)#[batchsize,1,max_len]
</code></pre>
<p>Luong Attention的实现：<br>
输入:<br>
input_step：每一步输入序列批次（一个单词）; shape =（1，batch_size）<br>
last_hidden：GRU的最终隐藏层; shape =（n_layers x num_directions，batch_size，hidden_size）<br>
encoder_outputs：编码器模型的输出; shape =（max_length，batch_size，hidden_size）<br>
输出:<br>
output: 一个softmax标准化后的张量， 代表了每个单词在解码序列中是下一个输出单词的概率; shape =（batch_size，voc.num_words）<br>
hidden: GRU的最终隐藏状态; shape =（n_layers x num_directions，batch_size，hidden_size）<br>
class LuongAttnDecoderRNN(nn.Module):<br>
def <strong>init</strong>(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):<br>
super(LuongAttnDecoderRNN, self).<strong>init</strong>()</p>
<pre><code>    # Keep for reference
    self.attn_model = attn_model
    self.hidden_size = hidden_size
    self.output_size = output_size
    self.n_layers = n_layers
    self.dropout = dropout

    # Define layers
    self.embedding = embedding
    self.embedding_dropout = nn.Dropout(dropout)
    self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))
    self.concat = nn.Linear(hidden_size * 2, hidden_size)
    self.out = nn.Linear(hidden_size, output_size)

    self.attn = Attn(attn_model, hidden_size)

def forward(self, input_step, last_hidden, encoder_outputs):
    # Note: we run this one step (word) at a time
    # Get embedding of current input word
    embedded = self.embedding(input_step)
    embedded = self.embedding_dropout(embedded)
    # Forward through unidirectional GRU
    rnn_output, hidden = self.gru(embedded, last_hidden)
    # Calculate attention weights from the current GRU output
    attn_weights = self.attn(rnn_output, encoder_outputs)
    # Multiply attention weights to encoder outputs to get new &quot;weighted sum&quot; context vector
    #权重维度＝[batchsize,1,max_len],encoder_outputs维度=[max_len,batchsize,hiddensize]
    context = attn_weights.bmm(encoder_outputs.transpose(0, 1))
    #context维度[batchsize,1,hiddensize]
    # Concatenate weighted context vector and GRU output using Luong eq. 5
    rnn_output = rnn_output.squeeze(0)
    context = context.squeeze(1)
    concat_input = torch.cat((rnn_output, context), 1)
    concat_output = torch.tanh(self.concat(concat_input))
    # Predict next word using Luong eq. 6
    output = self.out(concat_output)
    output = F.softmax(output, dim=1)
    # Return output and final hidden state
    return output, hidden
</code></pre>
<p>师兄代码：<br>
class RNN(nn.Module):<br>
def <strong>init</strong>(self, liner_size, n_units, cgi_dim, week_dim, hour_dim, cls_dim, cgi_size, batchsize, cls_ = 1):<br>
super(RNN, self).<strong>init</strong>()<br>
self.hidden_size = n_units<br>
self.batchsize = batchsize<br>
# self.add_module(&quot;batchnorm&quot;,nn.BatchNorm1d(n_units+em_dim))<br>
# self.add_module(&quot;lstm&quot;,nn.LSTM(liner_size + hour_dim + week_dim + cgi_dim, self.hidden_size, 2, dropout = dropout, batch_first = True))<br>
self.add_module(&quot;lstm&quot;,nn.LSTM(self.hidden_size, self.hidden_size, 2, batch_first = True))<br>
self.add_module(&quot;line_out&quot;,nn.Linear(self.hidden_size, cls_))<br>
self.hour_embedding = nn.Embedding(24, hour_dim)<br>
self.cgi_embedding = nn.Embedding(cgi_size, cgi_dim)<br>
self.week_embedding = nn.Embedding(7, week_dim)<br>
self.cls_embedding = nn.Embedding(2, cls_dim)<br>
self.hidden = self.init_hidden_state(self.batchsize)<br>
self.net = nn.Sequential(nn.Linear(1,liner_size), nn.PReLU())<br>
self.net2 = nn.Sequential(nn.Linear(liner_size + hour_dim, self.hidden_size), nn.PReLU())<br>
self.attn = nn.Sequential(nn.Tanh(), nn.Linear(self.hidden_size, 1, bias= False))<br>
#self.attn = nn.Sequential(nn.Linear(self.hidden_size, 1, bias= False))<br>
self.relu = nn.ReLU()</p>
<pre><code>def init_hidden_state(self, batchsize):
	return (Variable(torch.zeros(2,batchsize,self.hidden_size).cuda()),
			Variable(torch.zeros(2,batchsize,self.hidden_size).cuda()))


def forward(self, x, x_hour, x_cgi, x_week, x_cls, lookback): 
	x_s = self.net(torch.unsqueeze(x, 2))
	hour_ = self.hour_embedding(x_hour)
	#week_ = self.week_embedding(x_week)
	# cls_ = self.cls_embedding(x_cls)
	# print(x_s.size(),hour_.size(),cgi_.size(),week_.size())
	x_in = torch.cat((x_s, hour_),2)
	x_in = self.net2(x_in)
	# print(x_in.size())
	# pack = torch.nn.utils.rnn.pack_padded_sequence(x_in, lookback, batch_first=True)
	out , _ = self.lstm(x_in, self.hidden)#[128,32,64]
	#self.attn = nn.Sequential(nn.Tanh(), nn.Linear(self.hidden_size, 1, bias=False))
	attn_weights =  F.softmax(self.attn(out).transpose(1,2), dim = 2) # dim指定按对应维度计算softmax[128,1,32]
	context = torch.bmm(attn_weights, out)#[128,1,64]
            #线性层作decoder
	out_l = self.line_out(context)#[128,1,1]
	#out_l = self.relu(out_l)
	# unpacked = torch.nn.utils.rnn.pad_packed_sequence(out_l, batch_first = True)
	# y = unpacked[0]
	return out_l.squeeze()
</code></pre>

        </div>

        


        <div class="flex justify-between py-8">
          

          
            <div class="next-post">
              <a href="https://bailingnan.github.io//post/hello-world">
                <h3 class="post-title">
                  Hello World!
                  <i class="remixicon-arrow-right-line"></i>
                </h3>
              </a>
            </div>
          
        </div>

        
          
            <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<div id="gitalk-container"></div>

<script>

  var gitalk = new Gitalk({
    clientID: 'f4bd8cbd743b58235644',
    clientSecret: 'df6eff6e7836378726d917c527c47e86f1c30f8b',
    repo: 'bailingnan.github.io',
    owner: 'bailingnan',
    admin: ['bailingnan'],
    id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
  })

  gitalk.render('gitalk-container')

</script>

          

          
        

      </div>
    </div>

    <script src="https://bailingnan.github.io//media/prism.js"></script>  
<script>

Prism.highlightAll()

let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

// This should probably be throttled.
// Especially because it triggers during smooth scrolling.
// https://lodash.com/docs/4.17.10#throttle
// You could do like...
// window.addEventListener("scroll", () => {
//    _.throttle(doThatStuff, 100);
// });
// Only not doing it here to keep this Pen dependency-free.

window.addEventListener("scroll", event => {
  let fromTop = window.scrollY;

  mainNavLinks.forEach((link, index) => {
    let section = document.getElementById(decodeURI(link.hash).substring(1));
    let nextSection = null
    if (mainNavLinks[index + 1]) {
      nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
    }
    if (section.offsetTop <= fromTop) {
      if (nextSection) {
        if (nextSection.offsetTop > fromTop) {
          link.classList.add("current");
        } else {
          link.classList.remove("current");    
        }
      } else {
        link.classList.add("current");
      }
    } else {
      link.classList.remove("current");
    }
  });
});


document.addEventListener("DOMContentLoaded", function() {
  var lazyImages = [].slice.call(document.querySelectorAll(".post-feature-image.lazy"));

  if ("IntersectionObserver" in window) {
    let lazyImageObserver = new IntersectionObserver(function(entries, observer) {
      entries.forEach(function(entry) {
        if (entry.isIntersecting) {
          let lazyImage = entry.target
          lazyImage.style.backgroundImage = `url(${lazyImage.dataset.bg})`
          lazyImage.classList.remove("lazy")
          lazyImageObserver.unobserve(lazyImage)
        }
      });
    });

    lazyImages.forEach(function(lazyImage) {
      lazyImageObserver.observe(lazyImage)
    })
  } else {
    // Possibly fall back to a more compatible method here
  }
});

const menuContainer = document.querySelector('.menu-container')
const menus = document.querySelectorAll('.menu-container .menu')
const mask = document.querySelector('.mask')
const contentWrapper = document.querySelector('.content-wrapper')
const latestArticle = document.querySelector('.latest-article')
const readMore = document.querySelector('.read-more')
const indexPage = document.querySelector('.index-page')

const isHome = location.pathname === '/'
if (latestArticle) {
  latestArticle.style.display = isHome ? 'block' : 'none'
  readMore.style.display = isHome ? 'block' : 'none'
  indexPage.style.display = isHome ? 'none' : 'block'
}

const openMenu = () => {
  menuContainer.classList.add('open')
  menus.forEach(menu => {
    menu.classList.add('animated', 'fadeInLeft')
  })
  mask.classList.add('open')
  contentWrapper.classList.add('is-second')
}

const closeMenu = () => {
  menuContainer.classList.remove('open')
  menus.forEach(menu => {
    menu.classList.remove('animated', 'fadeInLeft')
  })
  mask.classList.remove('open')
  contentWrapper.classList.remove('is-second')
}
</script>
  
  </body>
</html>
