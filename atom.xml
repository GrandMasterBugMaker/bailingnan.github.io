<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://bailingnan.github.io/</id>
    <title>白凌南</title>
    <updated>2020-01-30T18:45:49.303Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://bailingnan.github.io/"/>
    <link rel="self" href="https://bailingnan.github.io/atom.xml"/>
    <subtitle>DL/RecSys/Python/Java/INTJ</subtitle>
    <logo>https://bailingnan.github.io/images/avatar.png</logo>
    <icon>https://bailingnan.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, 白凌南</rights>
    <entry>
        <title type="html"><![CDATA[Pycharm常用快捷键及技巧(macOS)]]></title>
        <id>https://bailingnan.github.io/post/pycharm-chang-yong-kuai-jie-jian-ji-ji-qiao-macos</id>
        <link href="https://bailingnan.github.io/post/pycharm-chang-yong-kuai-jie-jian-ji-ji-qiao-macos">
        </link>
        <updated>2020-01-30T18:37:40.000Z</updated>
        <content type="html"><![CDATA[<h1 id="mac键盘符号和修饰键说明">Mac键盘符号和修饰键说明</h1>
<ul>
<li><code>⌘</code>:Command</li>
<li><code>⇧</code>:Shift</li>
<li><code>⌥</code>:Option</li>
<li><code>⌃</code>:Control</li>
<li><code>↩︎</code>:Return/Enter</li>
<li><code>⌫</code>:Delete</li>
<li><code>⌦</code>:向前删除键（Fn+Delete）</li>
<li><code>↑</code>:上箭头</li>
<li><code>↓</code>:下箭头</li>
<li><code>←</code>:左箭头</li>
<li><code>→</code>:右箭头</li>
<li><code>⇞</code>:Page Up（Fn+↑）</li>
<li><code>⇟</code>:Page Down（Fn+↓）</li>
<li><code>Home</code>:Fn + ←</li>
<li><code>End</code>:Fn + →</li>
<li><code>⇥</code>:右制表符（Tab键）</li>
<li><code>⇤</code>:左制表符（Shift+Tab）</li>
<li><code>⎋</code>:Escape (Esc)</li>
<li>一直按住<code>fn</code>可调出F1~F10</li>
</ul>
<h1 id="editing编辑">Editing（编辑）</h1>
<ul>
<li><code>⌘Z</code>:撤销操作</li>
<li><code>⌘Y</code>:删除整行</li>
<li><code>⇧F6</code>:重命名文件</li>
<li><code>⌘S</code>:保存所有</li>
<li><code>⌦</code>:删除文件（Fn+Delete）</li>
<li><code>⌘⌥L</code>:格式化代码</li>
<li><code>⌘D</code>: 复制当前行或选定的块</li>
<li><code>⌘/</code>:注释/取消注释与行注释</li>
<li><code>⌘⌥/</code>:注释/取消注释与块注释</li>
<li><code>⌘J</code>:插入自定义动态代码模板</li>
<li><code>⌃Space</code>:基本的代码补全（补全任何类、方法、变量）</li>
<li><code>⌃⇧Space</code>:智能代码补全（过滤器方法列表和变量的预期类型）</li>
<li><code>⌘⇧↩</code>:自动结束代码，行末自动添加分号</li>
<li><code>⌘P</code>:显示方法的参数信息</li>
<li><code>⌃J</code>:快速查看文档</li>
<li><code>⇧F1</code>:查看外部文档（在某些代码上会触发打开浏览器显示相关文档）</li>
<li><code>⌘F1</code>:在错误或警告处显示具体描述信息</li>
<li><code>⌘N, ⌃↩, ⌃N</code>:生成代码（getter、setter、构造函数、hashCode/equals,toString）</li>
<li><code>⌥↑</code>:连续选中代码块</li>
<li><code>⌥↓</code>:减少当前选中的代码块</li>
<li><code>⌥↩</code>:显示意向动作和快速修复代码</li>
<li><code>⌘⇧] / ⌘⇧[</code>:选择直到代码块结束/开始</li>
<li><code>⌘+ / ⌘-</code>:展开 / 折叠代码块</li>
<li><code>⌘⇧+</code>:展开所有代码块</li>
<li><code>⌘⇧-</code>:折叠所有代码块</li>
</ul>
<h1 id="searchreplace查询替换">Search/Replace（查询/替换）</h1>
<ul>
<li><code>Double ⇧</code>:查询任何东西</li>
<li><code>⌘F</code>:文件内查找</li>
</ul>
<h1 id="compile-and-run编译和运行">Compile and Run（编译和运行）</h1>
<ul>
<li><code>⌃⇧F10</code>:Run</li>
<li><code>⌃⇧F9</code>:Debug</li>
</ul>
<h1 id="navigation导航">Navigation（导航）</h1>
<ul>
<li><code>⌘B</code>:进入光标所在的方法/变量的接口或是定义处</li>
<li><code>⌘⌥B</code>:跳转到实现处，在某个调用的方法名上使用会跳到具体的实现处，可以跳过接口</li>
<li><code>⌥ Space, ⌘Y</code>:快速打开光标所在方法、类的定义</li>
<li><code>⌃⇧B</code>:跳转到类型声明处</li>
<li><code>⌘U</code>:前往当前光标所在方法的父类的方法 / 接口定义</li>
<li><code>⌃H</code>:显示当前类的层次结构</li>
<li><code>⌘⇧H</code>:显示方法层次结构</li>
<li><code>⌃⌥H</code>:显示调用层次结构</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[PyTorch常用代码段]]></title>
        <id>https://bailingnan.github.io/post/pytorch-chang-yong-dai-ma-duan</id>
        <link href="https://bailingnan.github.io/post/pytorch-chang-yong-dai-ma-duan">
        </link>
        <updated>2020-01-27T10:51:08.000Z</updated>
        <summary type="html"><![CDATA[<h2 id="1基本配置">1.基本配置</h2>
<h3 id="导入包和版本查询">导入包和版本查询</h3>
<pre><code class="language-Python">import torch
import torch.nn as nn
import torchvision
print(torch.__version__)# PyTorch version
print(torch.version.cuda)#Corresponding CUDA version
print(torch.backends.cudnn.version())#Corresponding cuDNN version
print(torch.cuda.get_device_name(0))#GPU type
</code></pre>
<h3 id="可复现性">可复现性</h3>
<p>在硬件设备（CPU、GPU）不同时，完全的可复现性无法保证，即使随机种子相同。但是，在同一个设备上，应该保证可复现性。具体做法是，在程序开始的时候固定torch的随机种子，同时也把numpy的随机种子固定。</p>
<pre><code class="language-Python">np.random.seed(0)
torch.manual_seed(0)
torch.cuda.manual_seed_all(0)
</code></pre>
<h3 id="显卡设置">显卡设置</h3>
<p>如果只需要一张显卡</p>
<pre><code class="language-Python"># Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
</code></pre>
<p>如果需要指定多张显卡，比如0，1号显卡。</p>
<pre><code class="language-Python">import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
</code></pre>
<p>也可以在命令行运行代码时设置显卡：</p>
<pre><code>CUDA_VISIBLE_DEVICES=0,1 python train.py
</code></pre>
<p>清除显存:</p>
<pre><code class="language-Python">torch.cuda.empty_cache()
</code></pre>
<p>也可以使用在命令行重置GPU的指令：</p>
<pre><code>nvidia-smi --gpu-reset -i [gpu_id]
</code></pre>
<p>或在命令行可以先使用ps找到程序的PID，再使用kill结束该进程</p>
<pre><code class="language-python">ps aux | grep python
kill -9 [pid]
</code></pre>
<h3 id="设置为cudnn-benchmark模式">设置为cuDNN benchmark模式</h3>
<p>Benchmark模式会提升计算速度，但是由于计算中有随机性，每次网络前馈结果略有差异。</p>
<pre><code class="language-python">torch.backends.cudnn.benchmark = True
</code></pre>
<p>如果想要避免这种结果波动，设置</p>
<pre><code class="language-python">torch.backends.cudnn.deterministic = True
</code></pre>
<h2 id="张量tensor处理">张量(Tensor)处理</h2>
<h3 id="张量基本信息">张量基本信息</h3>
<pre><code class="language-Python">tensor = torch.randn(3,4,5)
print(tensor.type())  # 数据类型
print(tensor.size())  # 张量的shape，是个元组
print(tensor.dim())   # 维度的数量
</code></pre>
<h3 id="命名变量">命名变量</h3>
<pre><code class="language-Python"># 在PyTorch 1.3之前，需要使用注释
# Tensor[N, C, H, W]
images = torch.randn(32, 3, 56, 56)
images.sum(dim=1)
images.select(dim=1, index=0)

# PyTorch 1.3之后
NCHW = [‘N’, ‘C’, ‘H’, ‘W’]
images = torch.randn(32, 3, 56, 56, names=NCHW)
images.sum('C')
images.select('C', index=0)
# 也可以这么设置
tensor = torch.rand(3,4,1,2,names=('C', 'N', 'H', 'W'))
# 使用align_to可以对维度方便地排序
tensor = tensor.align_to('N', 'C', 'H', 'W')
</code></pre>
<h3 id="数据类型转换">数据类型转换</h3>
<pre><code class="language-Python"># 设置默认类型，pytorch中的FloatTensor远远快于DoubleTensor
torch.set_default_tensor_type(torch.FloatTensor)

# 类型转换
tensor = tensor.cuda()
tensor = tensor.cpu()
tensor = tensor.float()
tensor = tensor.long()
</code></pre>
<h3 id="torchtensor与npndarray转换">torch.Tensor与np.ndarray转换</h3>
<p>除了CharTensor，其他所有CPU上的张量都支持转换为numpy格式然后再转换回来。</p>
<pre><code class="language-Python">ndarray = tensor.cpu().numpy()
tensor = torch.from_numpy(ndarray).float()
tensor = torch.from_numpy(ndarray.copy()).float() # If ndarray has negative stride.
</code></pre>
<h3 id="从只包含一个元素的张量中提取值">从只包含一个元素的张量中提取值</h3>
<p><code>value = torch.rand(1).item()</code></p>
<h3 id="张量形变">张量形变</h3>
<pre><code class="language-Python"># 在将卷积层输入全连接层的情况下通常需要对张量做形变处理，
# 相比torch.view，torch.reshape可以自动处理输入张量不连续的情况。
tensor = torch.rand(2,3,4)
shape = (6, 4)
tensor = torch.reshape(tensor, shape)
</code></pre>
<h3 id="复制张量">复制张量</h3>
<pre><code class="language-Python"># Operation                 |  New/Shared memory | Still in computation graph |
tensor.clone()            # |        New         |          Yes               |
tensor.detach()           # |      Shared        |          No                |
tensor.detach.clone()()   # |        New         |          No                |
</code></pre>
<h3 id="张量拼接">张量拼接</h3>
<pre><code class="language-Python">'''
注意torch.cat和torch.stack的区别在于torch.cat沿着给定的维度拼接，
而torch.stack会新增一维。例如当参数是3个10x5的张量，torch.cat的结果是30x5的张量，
而torch.stack的结果是3x10x5的张量。
'''
tensor = torch.cat(list_of_tensors, dim=0)
tensor = torch.stack(list_of_tensors, dim=0)
</code></pre>
<h3 id="将整数标签转为one-hot编码">将整数标签转为one-hot编码</h3>
<pre><code class="language-python"># pytorch的标记默认从0开始
tensor = torch.tensor([0, 2, 1, 3])
N = tensor.size(0)
num_classes = 4
one_hot = torch.zeros(N, num_classes).long()
one_hot.scatter_(dim=1, index=torch.unsqueeze(tensor, dim=1), src=torch.ones(N, num_classes).long())
</code></pre>
<h3 id="得到非零元素">得到非零元素</h3>
<pre><code class="language-python">torch.nonzero(tensor)               # index of non-zero elements,包含点坐标的列表向量
torch.nonzero(tensor==0)            # index of zero elements
torch.nonzero(tensor).size(0)       # number of non-zero elements
torch.nonzero(tensor == 0).size(0)  # number of zero elements
</code></pre>
<h3 id="判断两个张量相等">判断两个张量相等</h3>
<pre><code class="language-Python">torch.allclose(tensor1, tensor2)  # float tensor
torch.equal(tensor1, tensor2)     # int tensor
</code></pre>
<h3 id="张量扩展">张量扩展</h3>
<pre><code class="language-python"># Expand tensor of shape 64*512 to shape 64*512*7*7.
tensor = torch.rand(64,512)
torch.reshape(tensor, (64, 512, 1, 1)).expand(64, 512, 7, 7)
</code></pre>
<h3 id="矩阵乘法">矩阵乘法</h3>
<pre><code class="language-python"># Matrix multiplcation: (m*n) * (n*p) * -&gt; (m*p).
result = torch.mm(tensor1, tensor2)

# Batch matrix multiplication: (b*m*n) * (b*n*p) -&gt; (b*m*p)
result = torch.bmm(tensor1, tensor2)

# Element-wise multiplication.
result = tensor1 * tensor2
</code></pre>
<h3 id="计算两组数据之间的两两欧式距离">计算两组数据之间的两两欧式距离</h3>
<p>利用broadcast机制</p>
<pre><code class="language-python">dist = torch.sqrt(torch.sum((X1[:,None,:] - X2) ** 2, dim=2))
</code></pre>
<h2 id="模型定义和操作">模型定义和操作</h2>
<h3 id="计算模型整体参数量">计算模型整体参数量</h3>
<pre><code class="language-python">num_parameters = sum(torch.numel(parameter) for parameter in model.parameters())
</code></pre>
<h3 id="查看网络中的参数">查看网络中的参数</h3>
<p>可以通过model.state_dict()或者model.named_parameters()函数查看现在的全部可训练参数（包括通过继承得到的父类中的参数）</p>
<pre><code class="language-python">params = list(model.named_parameters())
(name, param) = params[28]
print(name)
print(param.grad)
print('-------------------------------------------------')
(name2, param2) = params[29]
print(name2)
print(param2.grad)
print('----------------------------------------------------')
(name1, param1) = params[30]
print(name1)
print(param1.grad)
</code></pre>
<h3 id="类似-keras-的-modelsummary-输出模型信息使用pytorch-summary">类似 Keras 的 model.summary() 输出模型信息（使用pytorch-summary ）</h3>
<h3 id="模型权重初始化">模型权重初始化</h3>
<p>注意 model.modules() 和 model.children() 的区别：model.modules() 会迭代地遍历模型的所有子层，而 model.children() 只会遍历模型下的一层。</p>
<pre><code class="language-python"># Common practise for initialization.
for layer in model.modules():
    if isinstance(layer, torch.nn.Conv2d):
        torch.nn.init.kaiming_normal_(layer.weight, mode='fan_out',
                                      nonlinearity='relu')
        if layer.bias is not None:
            torch.nn.init.constant_(layer.bias, val=0.0)
    elif isinstance(layer, torch.nn.BatchNorm2d):
        torch.nn.init.constant_(layer.weight, val=1.0)
        torch.nn.init.constant_(layer.bias, val=0.0)
    elif isinstance(layer, torch.nn.Linear):
        torch.nn.init.xavier_normal_(layer.weight)
        if layer.bias is not None:
            torch.nn.init.constant_(layer.bias, val=0.0)

# Initialization with given tensor.
layer.weight = torch.nn.Parameter(tensor)
</code></pre>
<h3 id="提取模型中的某一层">提取模型中的某一层</h3>
<p>modules()会返回模型中所有模块的迭代器，它能够访问到最内层，比如self.layer1.conv1这个模块，还有一个与它们相对应的是name_children()属性以及named_modules(),这两个不仅会返回模块的迭代器，还会返回网络层的名字。</p>
<pre><code class="language-python"># 取模型中的前两层
new_model = nn.Sequential(*list(model.children())[:2] 
# 如果希望提取出模型中的所有卷积层，可以像下面这样操作：
for layer in model.named_modules():
    if isinstance(layer[1],nn.Conv2d):
         conv_model.add_module(layer[0],layer[1])
</code></pre>
<h3 id="部分层使用预训练模型">部分层使用预训练模型</h3>
<p>注意如果保存的模型是 torch.nn.DataParallel，则当前的模型也需要是</p>
<pre><code class="language-python">model.load_state_dict(torch.load('model.pth'), strict=False)
</code></pre>
<h2 id="模型训练和测试">模型训练和测试</h2>
<h3 id="自定义loss">自定义loss</h3>
<p>继承torch.nn.Module类写自己的loss。</p>
<pre><code class="language-python">class MyLoss(torch.nn.Moudle):
    def __init__(self):
        super(MyLoss, self).__init__()
        
    def forward(self, x, y):
        loss = torch.mean((x - y) ** 2)
        return loss
</code></pre>
<h3 id="标签平滑label-smoothing">标签平滑（label smoothing）</h3>
<p>写一个label_smoothing.py的文件，然后在训练代码里引用，用LSR代替交叉熵损失即可。label_smoothing.py内容如下：</p>
<pre><code class="language-python">import torch
import torch.nn as nn


class LSR(nn.Module):

    def __init__(self, e=0.1, reduction='mean'):
        super().__init__()

        self.log_softmax = nn.LogSoftmax(dim=1)
        self.e = e
        self.reduction = reduction
    
    def _one_hot(self, labels, classes, value=1):
        &quot;&quot;&quot;
            Convert labels to one hot vectors
        
        Args:
            labels: torch tensor in format [label1, label2, label3, ...]
            classes: int, number of classes
            value: label value in one hot vector, default to 1
        
        Returns:
            return one hot format labels in shape [batchsize, classes]
        &quot;&quot;&quot;

        one_hot = torch.zeros(labels.size(0), classes)

        #labels and value_added  size must match
        labels = labels.view(labels.size(0), -1)
        value_added = torch.Tensor(labels.size(0), 1).fill_(value)

        value_added = value_added.to(labels.device)
        one_hot = one_hot.to(labels.device)

        one_hot.scatter_add_(1, labels, value_added)

        return one_hot

    def _smooth_label(self, target, length, smooth_factor):
        &quot;&quot;&quot;convert targets to one-hot format, and smooth
        them.
        Args:
            target: target in form with [label1, label2, label_batchsize]
            length: length of one-hot format(number of classes)
            smooth_factor: smooth factor for label smooth
        
        Returns:
            smoothed labels in one hot format
        &quot;&quot;&quot;
        one_hot = self._one_hot(target, length, value=1 - smooth_factor)
        one_hot += smooth_factor / (length - 1)

        return one_hot.to(target.device)

    def forward(self, x, target):

        if x.size(0) != target.size(0):
            raise ValueError('Expected input batchsize ({}) to match target batch_size({})'
                    .format(x.size(0), target.size(0)))

        if x.dim() &lt; 2:
            raise ValueError('Expected input tensor to have least 2 dimensions(got {})'
                    .format(x.size(0)))

        if x.dim() != 2:
            raise ValueError('Only 2 dimension tensor are implemented, (got {})'
                    .format(x.size()))


        smoothed_target = self._smooth_label(target, x.size(1), self.e)
        x = self.log_softmax(x)
        loss = torch.sum(- x * smoothed_target, dim=1)

        if self.reduction == 'none':
            return loss
        
        elif self.reduction == 'sum':
            return torch.sum(loss)
        
        elif self.reduction == 'mean':
            return torch.mean(loss)
        
        else:
            raise ValueError('unrecognized option, expect reduction to be one of none, mean, sum')
</code></pre>
<p>或者直接在训练文件里做label smoothing</p>
<pre><code class="language-python">for images, labels in train_loader:
    images, labels = images.cuda(), labels.cuda()
    N = labels.size(0)
    # C is the number of classes.
    smoothed_labels = torch.full(size=(N, C), fill_value=0.1 / (C - 1)).cuda()
    smoothed_labels.scatter_(dim=1, index=torch.unsqueeze(labels, dim=1), value=0.9)

    score = model(images)
    log_prob = torch.nn.functional.log_softmax(score, dim=1)
    loss = -torch.sum(log_prob * smoothed_labels) / N
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()    
</code></pre>
<h3 id="l1-正则化">L1 正则化</h3>
<pre><code class="language-python">l1_regularization = torch.nn.L1Loss(reduction='sum')
loss = ...  # Standard cross-entropy loss
for param in model.parameters():
    loss += torch.sum(torch.abs(param))
loss.backward()
</code></pre>
<h3 id="不对偏置项进行权重衰减weight-decay">不对偏置项进行权重衰减（weight decay）</h3>
<p>pytorch里的weight decay相当于l2正则</p>
<pre><code class="language-python">bias_list = (param for name, param in model.named_parameters() if name[-4:] == 'bias')
others_list = (param for name, param in model.named_parameters() if name[-4:] != 'bias')
parameters = [{'parameters': bias_list, 'weight_decay': 0},                
              {'parameters': others_list}]
optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4)
</code></pre>
<h3 id="梯度裁剪gradient-clipping">梯度裁剪（gradient clipping）</h3>
<pre><code class="language-python">torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20)
</code></pre>
<h3 id="得到当前学习率">得到当前学习率</h3>
<pre><code class="language-python"># If there is one global learning rate (which is the common case).
lr = next(iter(optimizer.param_groups))['lr']

# If there are multiple learning rates for different layers.
all_lr = []
for param_group in optimizer.param_groups:
    all_lr.append(param_group['lr'])
</code></pre>
<p>另一种方法，在一个batch训练代码里，当前的lr是</p>
<pre><code class="language-python">optimizer.param_groups[0]['lr']
</code></pre>
<h3 id="学习率衰减">学习率衰减</h3>
<pre><code class="language-python"># Reduce learning rate when validation accuarcy plateau.
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, verbose=True)
for t in range(0, 80):
    train(...)
    val(...)
    scheduler.step(val_acc)

# Cosine annealing learning rate.
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=80)
# Reduce learning rate by 10 at given epochs.
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 70], gamma=0.1)
for t in range(0, 80):
    scheduler.step()    
    train(...)
    val(...)

# Learning rate warmup by 10 epochs.
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: t / 10)
for t in range(0, 10):
    scheduler.step()
    train(...)
    val(...)
</code></pre>
<h3 id="优化器链式更新">优化器链式更新</h3>
<p>从1.4版本开始，<code>torch.optim.lr_scheduler</code> 支持链式更新（chaining），即用户可以定义两个 schedulers，并交替在训练中使用。</p>
<pre><code class="language-python">import torch
from torch.optim import SGD
from torch.optim.lr_scheduler import ExponentialLR, StepLR
model = [torch.nn.Parameter(torch.randn(2, 2, requires_grad=True))]
optimizer = SGD(model, 0.1)
scheduler1 = ExponentialLR(optimizer, gamma=0.9)
scheduler2 = StepLR(optimizer, step_size=3, gamma=0.1)
for epoch in range(4):
    print(epoch, scheduler2.get_last_lr()[0])
    optimizer.step()
    scheduler1.step()
    scheduler2.step()
</code></pre>
<h3 id="模型训练可视化">模型训练可视化</h3>
<p>PyTorch可以使用tensorboard来可视化训练过程。<br>
安装和运行TensorBoard。</p>
<pre><code class="language-python">pip install tensorboard
tensorboard --logdir=runs
</code></pre>
<p>使用SummaryWriter类来收集和可视化相应的数据，放了方便查看，可以使用不同的文件夹，比如'Loss/train'和'Loss/test'。</p>
<pre><code class="language-python">from torch.utils.tensorboard import SummaryWriter
import numpy as np

writer = SummaryWriter()

for n_iter in range(100):
    writer.add_scalar('Loss/train', np.random.random(), n_iter)
    writer.add_scalar('Loss/test', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)
</code></pre>
<h3 id="保存与加载断点">保存与加载断点</h3>
<p>注意为了能够恢复训练，我们需要同时保存模型和优化器的状态，以及当前的训练轮数。</p>
<pre><code class="language-python">start_epoch = 0
# Load checkpoint.
if resume: # resume为参数，第一次训练时设为0，中断再训练时设为1
    model_path = os.path.join('model', 'best_checkpoint.pth.tar')
    assert os.path.isfile(model_path)
    checkpoint = torch.load(model_path)
    best_acc = checkpoint['best_acc']
    start_epoch = checkpoint['epoch']
    model.load_state_dict(checkpoint['model'])
    optimizer.load_state_dict(checkpoint['optimizer'])
    print('Load checkpoint at epoch {}.'.format(start_epoch))
    print('Best accuracy so far {}.'.format(best_acc))

# Train the model
for epoch in range(start_epoch, num_epochs): 
    ... 

    # Test the model
    ...
        
    # save checkpoint
    is_best = current_acc &gt; best_acc
    best_acc = max(current_acc, best_acc)
    checkpoint = {
        'best_acc': best_acc,
        'epoch': epoch + 1,
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict(),
    }
    model_path = os.path.join('model', 'checkpoint.pth.tar')
    best_model_path = os.path.join('model', 'best_checkpoint.pth.tar')
    torch.save(checkpoint, model_path)
    if is_best:
        shutil.copy(model_path, best_model_path)
</code></pre>
<h2 id="其他注意事项">其他注意事项</h2>
<ol>
<li>建议有参数的层和汇合（pooling）层使用<code>torch.nn</code>模块定义，激活函数直接使用<code>torch.nn.functional。torch.nn</code>模块和<code>torch.nn.functional</code>的区别在于，<code>torch.nn</code>模块在计算时底层调用了<code>torch.nn.functional</code>，但<code>torch.nn</code>模块包括该层参数，还可以应对训练和测试两种网络状态。使用<code>torch.nn.functional</code>时要注意网络状态，如</li>
</ol>
<pre><code class="language-python">def forward(self, x):
  ...
  x = torch.nn.functional.dropout(x, p=0.5, training=self.training)
</code></pre>
<ol start="2">
<li>不要使用太大的线性层。因为nn.Linear(m,n)使用的是O(mn)的内存，线性层太大很容易超出现有显存。</li>
<li>不要在太长的序列上使用RNN。因为RNN反向传播使用的是BPTT算法，其需要的内存和输入序列的长度呈线性关系。</li>
<li>model(x) 前用 <code>model.train()</code> 和 <code>model.eval()</code> 切换网络状态。</li>
<li>不需要计算梯度的代码块用 with torch.no_grad() 包含起来。</li>
<li><code>model.eval()</code> 和 <code>torch.no_grad()</code> 的区别在于，<code>model.eval()</code> 是将网络切换为测试状态，例如 BN 和dropout在训练和测试阶段使用不同的计算方法。<code>torch.no_grad()</code>是关闭 PyTorch 张量的自动求导机制，以减少存储使用和加速计算，得到的结果无法进行 <code>loss.backward()</code>。</li>
<li><code>model.zero_grad()</code>会把整个模型的参数的梯度都归零, 而<code>optimizer.zero_grad()</code>只会把传入其中的参数的梯度归零。</li>
<li>torch.nn.CrossEntropyLoss 的输入不需要经过 Softmax。torch.nn.CrossEntropyLoss 等价于 torch.nn.functional.log_softmax + torch.nn.NLLLoss。</li>
<li><code>loss.backward()</code>前用 <code>optimizer.zero_grad()</code> 清除累积梯度。</li>
<li><code>torch.utils.data.DataLoader</code> 中尽量设置 <code>pin_memory=True</code>，对特别小的数据集如 MNIST 设置 <code>pin_memory=False</code> 反而更快一些。<code>num_workers</code> 的设置需要在实验中找到最快的取值。</li>
<li>用 <code>del</code>及时删除不用的中间变量，节约 GPU 存储。</li>
<li>使用 <code>inplace</code> 操作可节约 GPU 存储，如</li>
</ol>
<pre><code class="language-python">x = torch.nn.functional.relu(x, inplace=True)
</code></pre>
<ol start="13">
<li>减少 CPU 和 GPU 之间的数据传输。例如如果你想知道一个 epoch 中每个 mini-batch 的 loss 和准确率，先将它们累积在 GPU 中等一个 epoch 结束之后一起传输回 CPU 会比每个 mini-batch 都进行一次 GPU 到 CPU 的传输更快。</li>
<li>使用半精度浮点数 <code>half()</code>会有一定的速度提升，具体效率依赖于 GPU 型号。需要小心数值精度过低带来的稳定性问题。</li>
<li>时常使用 <code>assert tensor.size() == (N, D, H, W)</code> 作为调试手段，确保张量维度和你设想中一致。</li>
<li>除了标记 y 外，尽量少使用一维张量，使用 n*1 的二维张量代替，可以避免一些意想不到的一维张量计算结果。</li>
<li>统计代码各部分耗时</li>
</ol>
<pre><code class="language-python">with torch.autograd.profiler.profile(enabled=True, use_cuda=False) as profile:
    ...
print(profile)

# 或者在命令行运行
python -m torch.utils.bottleneck main.py
</code></pre>
<ol start="18">
<li>使用TorchSnooper来调试PyTorch代码，程序在执行的时候，就会自动 print 出来每一行的执行结果的 tensor 的形状、数据类型、设备、是否需要梯度的信息。</li>
</ol>
<pre><code class="language-python"># pip install torchsnooper
import torchsnooper

# 对于函数，使用修饰器
@torchsnooper.snoop()

# 如果不是函数，使用 with 语句来激活 TorchSnooper，把训练的那个循环装进 with 语句中去。
with torchsnooper.snoop():
    原本的代码
</code></pre>
<ol start="19">
<li>模型可解释性，使用captum库</li>
</ol>
]]></summary>
        <content type="html"><![CDATA[<h2 id="1基本配置">1.基本配置</h2>
<h3 id="导入包和版本查询">导入包和版本查询</h3>
<pre><code class="language-Python">import torch
import torch.nn as nn
import torchvision
print(torch.__version__)# PyTorch version
print(torch.version.cuda)#Corresponding CUDA version
print(torch.backends.cudnn.version())#Corresponding cuDNN version
print(torch.cuda.get_device_name(0))#GPU type
</code></pre>
<h3 id="可复现性">可复现性</h3>
<p>在硬件设备（CPU、GPU）不同时，完全的可复现性无法保证，即使随机种子相同。但是，在同一个设备上，应该保证可复现性。具体做法是，在程序开始的时候固定torch的随机种子，同时也把numpy的随机种子固定。</p>
<pre><code class="language-Python">np.random.seed(0)
torch.manual_seed(0)
torch.cuda.manual_seed_all(0)
</code></pre>
<h3 id="显卡设置">显卡设置</h3>
<p>如果只需要一张显卡</p>
<pre><code class="language-Python"># Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
</code></pre>
<p>如果需要指定多张显卡，比如0，1号显卡。</p>
<pre><code class="language-Python">import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'
</code></pre>
<p>也可以在命令行运行代码时设置显卡：</p>
<pre><code>CUDA_VISIBLE_DEVICES=0,1 python train.py
</code></pre>
<p>清除显存:</p>
<pre><code class="language-Python">torch.cuda.empty_cache()
</code></pre>
<p>也可以使用在命令行重置GPU的指令：</p>
<pre><code>nvidia-smi --gpu-reset -i [gpu_id]
</code></pre>
<p>或在命令行可以先使用ps找到程序的PID，再使用kill结束该进程</p>
<pre><code class="language-python">ps aux | grep python
kill -9 [pid]
</code></pre>
<h3 id="设置为cudnn-benchmark模式">设置为cuDNN benchmark模式</h3>
<p>Benchmark模式会提升计算速度，但是由于计算中有随机性，每次网络前馈结果略有差异。</p>
<pre><code class="language-python">torch.backends.cudnn.benchmark = True
</code></pre>
<p>如果想要避免这种结果波动，设置</p>
<pre><code class="language-python">torch.backends.cudnn.deterministic = True
</code></pre>
<h2 id="张量tensor处理">张量(Tensor)处理</h2>
<h3 id="张量基本信息">张量基本信息</h3>
<pre><code class="language-Python">tensor = torch.randn(3,4,5)
print(tensor.type())  # 数据类型
print(tensor.size())  # 张量的shape，是个元组
print(tensor.dim())   # 维度的数量
</code></pre>
<h3 id="命名变量">命名变量</h3>
<pre><code class="language-Python"># 在PyTorch 1.3之前，需要使用注释
# Tensor[N, C, H, W]
images = torch.randn(32, 3, 56, 56)
images.sum(dim=1)
images.select(dim=1, index=0)

# PyTorch 1.3之后
NCHW = [‘N’, ‘C’, ‘H’, ‘W’]
images = torch.randn(32, 3, 56, 56, names=NCHW)
images.sum('C')
images.select('C', index=0)
# 也可以这么设置
tensor = torch.rand(3,4,1,2,names=('C', 'N', 'H', 'W'))
# 使用align_to可以对维度方便地排序
tensor = tensor.align_to('N', 'C', 'H', 'W')
</code></pre>
<h3 id="数据类型转换">数据类型转换</h3>
<pre><code class="language-Python"># 设置默认类型，pytorch中的FloatTensor远远快于DoubleTensor
torch.set_default_tensor_type(torch.FloatTensor)

# 类型转换
tensor = tensor.cuda()
tensor = tensor.cpu()
tensor = tensor.float()
tensor = tensor.long()
</code></pre>
<h3 id="torchtensor与npndarray转换">torch.Tensor与np.ndarray转换</h3>
<p>除了CharTensor，其他所有CPU上的张量都支持转换为numpy格式然后再转换回来。</p>
<pre><code class="language-Python">ndarray = tensor.cpu().numpy()
tensor = torch.from_numpy(ndarray).float()
tensor = torch.from_numpy(ndarray.copy()).float() # If ndarray has negative stride.
</code></pre>
<h3 id="从只包含一个元素的张量中提取值">从只包含一个元素的张量中提取值</h3>
<p><code>value = torch.rand(1).item()</code></p>
<h3 id="张量形变">张量形变</h3>
<pre><code class="language-Python"># 在将卷积层输入全连接层的情况下通常需要对张量做形变处理，
# 相比torch.view，torch.reshape可以自动处理输入张量不连续的情况。
tensor = torch.rand(2,3,4)
shape = (6, 4)
tensor = torch.reshape(tensor, shape)
</code></pre>
<h3 id="复制张量">复制张量</h3>
<pre><code class="language-Python"># Operation                 |  New/Shared memory | Still in computation graph |
tensor.clone()            # |        New         |          Yes               |
tensor.detach()           # |      Shared        |          No                |
tensor.detach.clone()()   # |        New         |          No                |
</code></pre>
<h3 id="张量拼接">张量拼接</h3>
<pre><code class="language-Python">'''
注意torch.cat和torch.stack的区别在于torch.cat沿着给定的维度拼接，
而torch.stack会新增一维。例如当参数是3个10x5的张量，torch.cat的结果是30x5的张量，
而torch.stack的结果是3x10x5的张量。
'''
tensor = torch.cat(list_of_tensors, dim=0)
tensor = torch.stack(list_of_tensors, dim=0)
</code></pre>
<h3 id="将整数标签转为one-hot编码">将整数标签转为one-hot编码</h3>
<pre><code class="language-python"># pytorch的标记默认从0开始
tensor = torch.tensor([0, 2, 1, 3])
N = tensor.size(0)
num_classes = 4
one_hot = torch.zeros(N, num_classes).long()
one_hot.scatter_(dim=1, index=torch.unsqueeze(tensor, dim=1), src=torch.ones(N, num_classes).long())
</code></pre>
<h3 id="得到非零元素">得到非零元素</h3>
<pre><code class="language-python">torch.nonzero(tensor)               # index of non-zero elements,包含点坐标的列表向量
torch.nonzero(tensor==0)            # index of zero elements
torch.nonzero(tensor).size(0)       # number of non-zero elements
torch.nonzero(tensor == 0).size(0)  # number of zero elements
</code></pre>
<h3 id="判断两个张量相等">判断两个张量相等</h3>
<pre><code class="language-Python">torch.allclose(tensor1, tensor2)  # float tensor
torch.equal(tensor1, tensor2)     # int tensor
</code></pre>
<h3 id="张量扩展">张量扩展</h3>
<pre><code class="language-python"># Expand tensor of shape 64*512 to shape 64*512*7*7.
tensor = torch.rand(64,512)
torch.reshape(tensor, (64, 512, 1, 1)).expand(64, 512, 7, 7)
</code></pre>
<h3 id="矩阵乘法">矩阵乘法</h3>
<pre><code class="language-python"># Matrix multiplcation: (m*n) * (n*p) * -&gt; (m*p).
result = torch.mm(tensor1, tensor2)

# Batch matrix multiplication: (b*m*n) * (b*n*p) -&gt; (b*m*p)
result = torch.bmm(tensor1, tensor2)

# Element-wise multiplication.
result = tensor1 * tensor2
</code></pre>
<h3 id="计算两组数据之间的两两欧式距离">计算两组数据之间的两两欧式距离</h3>
<p>利用broadcast机制</p>
<pre><code class="language-python">dist = torch.sqrt(torch.sum((X1[:,None,:] - X2) ** 2, dim=2))
</code></pre>
<h2 id="模型定义和操作">模型定义和操作</h2>
<h3 id="计算模型整体参数量">计算模型整体参数量</h3>
<pre><code class="language-python">num_parameters = sum(torch.numel(parameter) for parameter in model.parameters())
</code></pre>
<h3 id="查看网络中的参数">查看网络中的参数</h3>
<p>可以通过model.state_dict()或者model.named_parameters()函数查看现在的全部可训练参数（包括通过继承得到的父类中的参数）</p>
<pre><code class="language-python">params = list(model.named_parameters())
(name, param) = params[28]
print(name)
print(param.grad)
print('-------------------------------------------------')
(name2, param2) = params[29]
print(name2)
print(param2.grad)
print('----------------------------------------------------')
(name1, param1) = params[30]
print(name1)
print(param1.grad)
</code></pre>
<h3 id="类似-keras-的-modelsummary-输出模型信息使用pytorch-summary">类似 Keras 的 model.summary() 输出模型信息（使用pytorch-summary ）</h3>
<h3 id="模型权重初始化">模型权重初始化</h3>
<p>注意 model.modules() 和 model.children() 的区别：model.modules() 会迭代地遍历模型的所有子层，而 model.children() 只会遍历模型下的一层。</p>
<pre><code class="language-python"># Common practise for initialization.
for layer in model.modules():
    if isinstance(layer, torch.nn.Conv2d):
        torch.nn.init.kaiming_normal_(layer.weight, mode='fan_out',
                                      nonlinearity='relu')
        if layer.bias is not None:
            torch.nn.init.constant_(layer.bias, val=0.0)
    elif isinstance(layer, torch.nn.BatchNorm2d):
        torch.nn.init.constant_(layer.weight, val=1.0)
        torch.nn.init.constant_(layer.bias, val=0.0)
    elif isinstance(layer, torch.nn.Linear):
        torch.nn.init.xavier_normal_(layer.weight)
        if layer.bias is not None:
            torch.nn.init.constant_(layer.bias, val=0.0)

# Initialization with given tensor.
layer.weight = torch.nn.Parameter(tensor)
</code></pre>
<h3 id="提取模型中的某一层">提取模型中的某一层</h3>
<p>modules()会返回模型中所有模块的迭代器，它能够访问到最内层，比如self.layer1.conv1这个模块，还有一个与它们相对应的是name_children()属性以及named_modules(),这两个不仅会返回模块的迭代器，还会返回网络层的名字。</p>
<pre><code class="language-python"># 取模型中的前两层
new_model = nn.Sequential(*list(model.children())[:2] 
# 如果希望提取出模型中的所有卷积层，可以像下面这样操作：
for layer in model.named_modules():
    if isinstance(layer[1],nn.Conv2d):
         conv_model.add_module(layer[0],layer[1])
</code></pre>
<h3 id="部分层使用预训练模型">部分层使用预训练模型</h3>
<p>注意如果保存的模型是 torch.nn.DataParallel，则当前的模型也需要是</p>
<pre><code class="language-python">model.load_state_dict(torch.load('model.pth'), strict=False)
</code></pre>
<h2 id="模型训练和测试">模型训练和测试</h2>
<h3 id="自定义loss">自定义loss</h3>
<p>继承torch.nn.Module类写自己的loss。</p>
<pre><code class="language-python">class MyLoss(torch.nn.Moudle):
    def __init__(self):
        super(MyLoss, self).__init__()
        
    def forward(self, x, y):
        loss = torch.mean((x - y) ** 2)
        return loss
</code></pre>
<h3 id="标签平滑label-smoothing">标签平滑（label smoothing）</h3>
<p>写一个label_smoothing.py的文件，然后在训练代码里引用，用LSR代替交叉熵损失即可。label_smoothing.py内容如下：</p>
<pre><code class="language-python">import torch
import torch.nn as nn


class LSR(nn.Module):

    def __init__(self, e=0.1, reduction='mean'):
        super().__init__()

        self.log_softmax = nn.LogSoftmax(dim=1)
        self.e = e
        self.reduction = reduction
    
    def _one_hot(self, labels, classes, value=1):
        &quot;&quot;&quot;
            Convert labels to one hot vectors
        
        Args:
            labels: torch tensor in format [label1, label2, label3, ...]
            classes: int, number of classes
            value: label value in one hot vector, default to 1
        
        Returns:
            return one hot format labels in shape [batchsize, classes]
        &quot;&quot;&quot;

        one_hot = torch.zeros(labels.size(0), classes)

        #labels and value_added  size must match
        labels = labels.view(labels.size(0), -1)
        value_added = torch.Tensor(labels.size(0), 1).fill_(value)

        value_added = value_added.to(labels.device)
        one_hot = one_hot.to(labels.device)

        one_hot.scatter_add_(1, labels, value_added)

        return one_hot

    def _smooth_label(self, target, length, smooth_factor):
        &quot;&quot;&quot;convert targets to one-hot format, and smooth
        them.
        Args:
            target: target in form with [label1, label2, label_batchsize]
            length: length of one-hot format(number of classes)
            smooth_factor: smooth factor for label smooth
        
        Returns:
            smoothed labels in one hot format
        &quot;&quot;&quot;
        one_hot = self._one_hot(target, length, value=1 - smooth_factor)
        one_hot += smooth_factor / (length - 1)

        return one_hot.to(target.device)

    def forward(self, x, target):

        if x.size(0) != target.size(0):
            raise ValueError('Expected input batchsize ({}) to match target batch_size({})'
                    .format(x.size(0), target.size(0)))

        if x.dim() &lt; 2:
            raise ValueError('Expected input tensor to have least 2 dimensions(got {})'
                    .format(x.size(0)))

        if x.dim() != 2:
            raise ValueError('Only 2 dimension tensor are implemented, (got {})'
                    .format(x.size()))


        smoothed_target = self._smooth_label(target, x.size(1), self.e)
        x = self.log_softmax(x)
        loss = torch.sum(- x * smoothed_target, dim=1)

        if self.reduction == 'none':
            return loss
        
        elif self.reduction == 'sum':
            return torch.sum(loss)
        
        elif self.reduction == 'mean':
            return torch.mean(loss)
        
        else:
            raise ValueError('unrecognized option, expect reduction to be one of none, mean, sum')
</code></pre>
<p>或者直接在训练文件里做label smoothing</p>
<pre><code class="language-python">for images, labels in train_loader:
    images, labels = images.cuda(), labels.cuda()
    N = labels.size(0)
    # C is the number of classes.
    smoothed_labels = torch.full(size=(N, C), fill_value=0.1 / (C - 1)).cuda()
    smoothed_labels.scatter_(dim=1, index=torch.unsqueeze(labels, dim=1), value=0.9)

    score = model(images)
    log_prob = torch.nn.functional.log_softmax(score, dim=1)
    loss = -torch.sum(log_prob * smoothed_labels) / N
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()    
</code></pre>
<h3 id="l1-正则化">L1 正则化</h3>
<pre><code class="language-python">l1_regularization = torch.nn.L1Loss(reduction='sum')
loss = ...  # Standard cross-entropy loss
for param in model.parameters():
    loss += torch.sum(torch.abs(param))
loss.backward()
</code></pre>
<h3 id="不对偏置项进行权重衰减weight-decay">不对偏置项进行权重衰减（weight decay）</h3>
<p>pytorch里的weight decay相当于l2正则</p>
<pre><code class="language-python">bias_list = (param for name, param in model.named_parameters() if name[-4:] == 'bias')
others_list = (param for name, param in model.named_parameters() if name[-4:] != 'bias')
parameters = [{'parameters': bias_list, 'weight_decay': 0},                
              {'parameters': others_list}]
optimizer = torch.optim.SGD(parameters, lr=1e-2, momentum=0.9, weight_decay=1e-4)
</code></pre>
<h3 id="梯度裁剪gradient-clipping">梯度裁剪（gradient clipping）</h3>
<pre><code class="language-python">torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20)
</code></pre>
<h3 id="得到当前学习率">得到当前学习率</h3>
<pre><code class="language-python"># If there is one global learning rate (which is the common case).
lr = next(iter(optimizer.param_groups))['lr']

# If there are multiple learning rates for different layers.
all_lr = []
for param_group in optimizer.param_groups:
    all_lr.append(param_group['lr'])
</code></pre>
<p>另一种方法，在一个batch训练代码里，当前的lr是</p>
<pre><code class="language-python">optimizer.param_groups[0]['lr']
</code></pre>
<h3 id="学习率衰减">学习率衰减</h3>
<pre><code class="language-python"># Reduce learning rate when validation accuarcy plateau.
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, verbose=True)
for t in range(0, 80):
    train(...)
    val(...)
    scheduler.step(val_acc)

# Cosine annealing learning rate.
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=80)
# Reduce learning rate by 10 at given epochs.
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 70], gamma=0.1)
for t in range(0, 80):
    scheduler.step()    
    train(...)
    val(...)

# Learning rate warmup by 10 epochs.
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: t / 10)
for t in range(0, 10):
    scheduler.step()
    train(...)
    val(...)
</code></pre>
<h3 id="优化器链式更新">优化器链式更新</h3>
<p>从1.4版本开始，<code>torch.optim.lr_scheduler</code> 支持链式更新（chaining），即用户可以定义两个 schedulers，并交替在训练中使用。</p>
<pre><code class="language-python">import torch
from torch.optim import SGD
from torch.optim.lr_scheduler import ExponentialLR, StepLR
model = [torch.nn.Parameter(torch.randn(2, 2, requires_grad=True))]
optimizer = SGD(model, 0.1)
scheduler1 = ExponentialLR(optimizer, gamma=0.9)
scheduler2 = StepLR(optimizer, step_size=3, gamma=0.1)
for epoch in range(4):
    print(epoch, scheduler2.get_last_lr()[0])
    optimizer.step()
    scheduler1.step()
    scheduler2.step()
</code></pre>
<h3 id="模型训练可视化">模型训练可视化</h3>
<p>PyTorch可以使用tensorboard来可视化训练过程。<br>
安装和运行TensorBoard。</p>
<pre><code class="language-python">pip install tensorboard
tensorboard --logdir=runs
</code></pre>
<p>使用SummaryWriter类来收集和可视化相应的数据，放了方便查看，可以使用不同的文件夹，比如'Loss/train'和'Loss/test'。</p>
<pre><code class="language-python">from torch.utils.tensorboard import SummaryWriter
import numpy as np

writer = SummaryWriter()

for n_iter in range(100):
    writer.add_scalar('Loss/train', np.random.random(), n_iter)
    writer.add_scalar('Loss/test', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)
    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)
</code></pre>
<h3 id="保存与加载断点">保存与加载断点</h3>
<p>注意为了能够恢复训练，我们需要同时保存模型和优化器的状态，以及当前的训练轮数。</p>
<pre><code class="language-python">start_epoch = 0
# Load checkpoint.
if resume: # resume为参数，第一次训练时设为0，中断再训练时设为1
    model_path = os.path.join('model', 'best_checkpoint.pth.tar')
    assert os.path.isfile(model_path)
    checkpoint = torch.load(model_path)
    best_acc = checkpoint['best_acc']
    start_epoch = checkpoint['epoch']
    model.load_state_dict(checkpoint['model'])
    optimizer.load_state_dict(checkpoint['optimizer'])
    print('Load checkpoint at epoch {}.'.format(start_epoch))
    print('Best accuracy so far {}.'.format(best_acc))

# Train the model
for epoch in range(start_epoch, num_epochs): 
    ... 

    # Test the model
    ...
        
    # save checkpoint
    is_best = current_acc &gt; best_acc
    best_acc = max(current_acc, best_acc)
    checkpoint = {
        'best_acc': best_acc,
        'epoch': epoch + 1,
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict(),
    }
    model_path = os.path.join('model', 'checkpoint.pth.tar')
    best_model_path = os.path.join('model', 'best_checkpoint.pth.tar')
    torch.save(checkpoint, model_path)
    if is_best:
        shutil.copy(model_path, best_model_path)
</code></pre>
<h2 id="其他注意事项">其他注意事项</h2>
<ol>
<li>建议有参数的层和汇合（pooling）层使用<code>torch.nn</code>模块定义，激活函数直接使用<code>torch.nn.functional。torch.nn</code>模块和<code>torch.nn.functional</code>的区别在于，<code>torch.nn</code>模块在计算时底层调用了<code>torch.nn.functional</code>，但<code>torch.nn</code>模块包括该层参数，还可以应对训练和测试两种网络状态。使用<code>torch.nn.functional</code>时要注意网络状态，如</li>
</ol>
<pre><code class="language-python">def forward(self, x):
  ...
  x = torch.nn.functional.dropout(x, p=0.5, training=self.training)
</code></pre>
<ol start="2">
<li>不要使用太大的线性层。因为nn.Linear(m,n)使用的是O(mn)的内存，线性层太大很容易超出现有显存。</li>
<li>不要在太长的序列上使用RNN。因为RNN反向传播使用的是BPTT算法，其需要的内存和输入序列的长度呈线性关系。</li>
<li>model(x) 前用 <code>model.train()</code> 和 <code>model.eval()</code> 切换网络状态。</li>
<li>不需要计算梯度的代码块用 with torch.no_grad() 包含起来。</li>
<li><code>model.eval()</code> 和 <code>torch.no_grad()</code> 的区别在于，<code>model.eval()</code> 是将网络切换为测试状态，例如 BN 和dropout在训练和测试阶段使用不同的计算方法。<code>torch.no_grad()</code>是关闭 PyTorch 张量的自动求导机制，以减少存储使用和加速计算，得到的结果无法进行 <code>loss.backward()</code>。</li>
<li><code>model.zero_grad()</code>会把整个模型的参数的梯度都归零, 而<code>optimizer.zero_grad()</code>只会把传入其中的参数的梯度归零。</li>
<li>torch.nn.CrossEntropyLoss 的输入不需要经过 Softmax。torch.nn.CrossEntropyLoss 等价于 torch.nn.functional.log_softmax + torch.nn.NLLLoss。</li>
<li><code>loss.backward()</code>前用 <code>optimizer.zero_grad()</code> 清除累积梯度。</li>
<li><code>torch.utils.data.DataLoader</code> 中尽量设置 <code>pin_memory=True</code>，对特别小的数据集如 MNIST 设置 <code>pin_memory=False</code> 反而更快一些。<code>num_workers</code> 的设置需要在实验中找到最快的取值。</li>
<li>用 <code>del</code>及时删除不用的中间变量，节约 GPU 存储。</li>
<li>使用 <code>inplace</code> 操作可节约 GPU 存储，如</li>
</ol>
<pre><code class="language-python">x = torch.nn.functional.relu(x, inplace=True)
</code></pre>
<ol start="13">
<li>减少 CPU 和 GPU 之间的数据传输。例如如果你想知道一个 epoch 中每个 mini-batch 的 loss 和准确率，先将它们累积在 GPU 中等一个 epoch 结束之后一起传输回 CPU 会比每个 mini-batch 都进行一次 GPU 到 CPU 的传输更快。</li>
<li>使用半精度浮点数 <code>half()</code>会有一定的速度提升，具体效率依赖于 GPU 型号。需要小心数值精度过低带来的稳定性问题。</li>
<li>时常使用 <code>assert tensor.size() == (N, D, H, W)</code> 作为调试手段，确保张量维度和你设想中一致。</li>
<li>除了标记 y 外，尽量少使用一维张量，使用 n*1 的二维张量代替，可以避免一些意想不到的一维张量计算结果。</li>
<li>统计代码各部分耗时</li>
</ol>
<pre><code class="language-python">with torch.autograd.profiler.profile(enabled=True, use_cuda=False) as profile:
    ...
print(profile)

# 或者在命令行运行
python -m torch.utils.bottleneck main.py
</code></pre>
<ol start="18">
<li>使用TorchSnooper来调试PyTorch代码，程序在执行的时候，就会自动 print 出来每一行的执行结果的 tensor 的形状、数据类型、设备、是否需要梯度的信息。</li>
</ol>
<pre><code class="language-python"># pip install torchsnooper
import torchsnooper

# 对于函数，使用修饰器
@torchsnooper.snoop()

# 如果不是函数，使用 with 语句来激活 TorchSnooper，把训练的那个循环装进 with 语句中去。
with torchsnooper.snoop():
    原本的代码
</code></pre>
<ol start="19">
<li>模型可解释性，使用captum库</li>
</ol>
<!-- more -->
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linux常用命令]]></title>
        <id>https://bailingnan.github.io/post/linux-chang-yong-ming-ling</id>
        <link href="https://bailingnan.github.io/post/linux-chang-yong-ming-ling">
        </link>
        <updated>2020-01-22T08:16:23.000Z</updated>
        <content type="html"><![CDATA[<h1 id="一-linux文件与目录">一、Linux文件与目录</h1>
<ul>
<li>
<h2 id="11-linux文件权限概念">1.1 Linux文件权限概念</h2>
<ul>
<li>切换至root身份:<code>su -</code></li>
<li>离开root身份:<code>exit</code></li>
<li>文件名第一个字符为.的文件为隐藏文件</li>
</ul>
</li>
<li>
<h2 id="12-linux目录配置">1.2 Linux目录配置</h2>
</li>
<li>
<p>FHS针对目录树架构仅定义出三层目录下面应该放置什么数据而已，分别是下面这三个目录的定义：</p>
<ul>
<li><code>/</code>（root, 根目录）:与开机系统有关；</li>
<li><code>/usr</code> （unix software resource）:与软件安装/执行有关；</li>
<li><code>/var</code>（variable）:与系统运行过程有关。</li>
</ul>
</li>
<li>
<p>FHS要求根目录中必须存在的目录:</p>
<ul>
<li>
<p><code>/bin</code>:系统有很多放置可执行文件的目录，但<code>/bin</code>比较特殊。因为<code>/bin</code>放置的是在 单人维护模式下还能够被操作的指令。 在<code>/bin</code>下面的指令可以被root与一般帐号所使用，主要有：cat, chmod, chown, date, mv, mkdir, cp, bash等等常用的指令。</p>
</li>
<li>
<p><code>/boot</code>:主要在放置开机会使用到的文件，包括 Linux 核心文件以及开机菜单与开机所需配置文件等等Linux kernel 常用的文件名为：vmlinuz，如果使用的是 grub2 这个开机管理程序， 则还会存在<code>/boot/grub2/</code>这个目录。</p>
</li>
<li>
<p><code>/dev</code>:存放硬件相关的文件。在 Linux 系统上，任何设备与周边设备都是以文件的型态存在于这个目录当中的。你只要通过存取这个目录下面的某个文件，就等于存取某个设备。比要重要的文件有<code>/dev/null</code>, <code>/dev/zero</code>, <code>/dev/tty</code>, <code>/dev/loop</code>, <code>/dev/sd</code>等等。</p>
</li>
<li>
<p><code>/etc</code>:存放配置文件的目录。系统主要的配置文件几乎都放置在这个目录内，例如人员的帐号密码档、各种服务的启始档等等。一般来说，这个目录下的各文件属性是可以让一般使用者查阅的，但是只有 root 有权力修改。FHS建议不要放置可执行文件 （binary）在这个目录中喔。比较重要的文件有： <code>/etc/modprobe.d/</code>, <code>/etc/passwd</code>, <code>/etc/fstab</code>, <code>/etc/issue</code>等等。另外 FHS 还规范几个重要的目录最好要存在 <code>/etc/</code>目录下：</p>
<ul>
<li><code>/etc/opt</code>（必要）：这个目录在放置第三方协力软件 <code>/opt</code> 的相关配置文件。</li>
<li><code>/etc/X11/</code>（建议）：与 X Window 有关的各种配置文件都在这里，尤其是 xorg.conf 这个 X Server 的配置文件。</li>
<li><code>/etc/sgml/</code>（建议）：与 SGML 格式有关的各项配置文件。</li>
<li><code>/etc/xml/</code>（建议）：与 XML 格式有关的各项配置文件。</li>
</ul>
</li>
<li>
<p><code>/lib</code>:系统的函数库非常的多，而<code>/lib</code>放置的则是在开机时会用到的函数库， 以及 在<code>/bin</code>或<code>/sbin</code>下面的指令会调用的函数库而已。 什么是函数库呢？你可以将 他想成是“外挂”，某些指令必须要有这些“外挂”才能够顺利完成程序的执行 之意。 另外 FSH 还要求下面的目录必须要存在：<code>/lib/modules/</code>：这个目录 主要放置可抽换式的核心相关模块（驱动程序）</p>
</li>
<li>
<p><code>/media</code>:media是“媒体”的英文，顾名思义，这个<code>/media</code>下面放置的就是可移除的设备！ 包括软盘、光盘、DVD等等设备都暂时挂载于此。常见的文件名 有：<code>/media/floppy</code>, <code>/media/cdrom</code>等等。</p>
</li>
<li>
<p><code>/mnt</code>:如果你想要暂时挂载某些额外的设备，一般建议你可以放置到这个目录中。 在古早时候，这个目录的用途与<code>/media</code>相同啦！只是有了<code>/media</code>之后，这个 目录就用来暂时挂载用了。</p>
</li>
<li>
<p><code>/opt</code>:这个是给第三方协力软件放置的目录。什么是第三方协力软件啊？ 举例来说，KDE 这个桌面管理系统是一个独立的计划，不过他可以安装到 Linux 系统中，因此KDE的软件就建议放置到此目录下了。 另外，如果你想要自行安装额外的软件（非原本的 distribution 提供的），那么也能够将你的软件安装到这里来。 不过，以前的 Linux 系统中，我们还是习惯放置在<code>/usr/local</code>目录下呢！</p>
</li>
<li>
<p><code>/run</code>:早期的 FHS 规定系统开机后所产生的各项信息应该要放置到 <code>/var/run</code> 目录下，新版的 FHS 则规范到 <code>/run</code> 下面。 由于 <code>/run</code> 可以使用内存来仿真，因此性能上会好很多！</p>
</li>
<li>
<p><code>/sbin</code>:存放管理员root可以执行的命令。Linux 有非常多指令是用来设置系统环境的，这些指令只有 root 才能够利用 来“设置”系统，其他使用者最多只能用来“查询”而已。放在<code>/sbin</code>下面的为开机过程中所需要的，里面包括了开机、修复、还原系统所需要的指令。至于某些服务器软件程序，一般则放置到<code>/usr/sbin/</code>当中。至于本机自行安装的软件所产生的系统可执行文件（system binary）， 则放置到<code>/usr/local/sbin/</code> 当中了。常见的指令包括：fdisk, fsck, ifconfig, mkfs等等。</p>
</li>
<li>
<p><code>/srv</code>:srv 可以视为“service”的缩写，是一些网络服务启动之后，这些服务所需要取用的数据目录。 常见的服务例如 WWW, FTP 等等。举例来说，WWW 服务器需要的网页数据就可以放置在<code>/srv/www/</code>里面。 不过，系统的服务数据 如果尚未要提供给网际网络任何人浏览的话，默认还是建议放置到 <code>/var/lib</code>下面即可。</p>
</li>
<li>
<p><code>tem</code>:这是让一般使用者或者是正在执行的程序暂时放置文件的地方。 这个目录是任何人都能够存取的，所以你需要定期的清理一下。当然，重要数据不可放置在此目录啊！ 因为FHS甚至建议在开机时，应该要将<code>/tmp</code>下的数据都删除唷!</p>
</li>
<li>
<p><code>/var</code>:第二层 FHS 设置，主要为放置变动性的数据，后续介绍。</p>
</li>
<li>
<p><code>/usr</code>:第二层 FHS 设置，后续介绍。</p>
</li>
</ul>
</li>
<li>
<p>FHS 建议根目录中可以存在的目录：</p>
<ul>
<li><code>/home</code>：这是系统默认的使用者主文件夹（home directory）。在你新增一个一般使 用者帐号时，默认的使用者主文件夹都会规范到这里来。</li>
<li><code>/lib&lt;equal&gt;</code>:用来存放与 <code>/lib</code> 不同的格式的二进制函数库，例如支持 64 位的 <code>/lib64</code> 函数库等。</li>
<li><code>/root</code>:系统管理员（root）的主文件夹。之所以放在这里，是因为如果进入单人维护模式而仅挂载根目录时， 该目录就能够拥有 root 的主文件夹，所以我们会 希望 root 的主文件夹与根目录放置在同一个分区中。</li>
<li><code>/lost+found</code>:这个目录是使用标准的 ext2/ext3/ext4 文件系统格式才会产生的一个目录， 目的在于当文件系统发生错误时， 将一些遗失的片段放置到这个目录下。 不过如果使用的是 xfs 文件系统的话，就不会存在这个目录了！</li>
<li><code>/proc</code>:这个目录本身是一个“虚拟文件系统（virtual filesystem）”喔！他放置的数据都是在内存当中， 例如系统核心、行程信息（process）、周边设备的状态及网络状态等等。因为这个目录下的数据都是在内存当中， 所以本身不占任何硬盘空间啊！比较重要的文件例如：<code>/proc/cpuinfo</code>, <code>/proc/dma</code>, <code>/proc/interrupts</code>, <code>/proc/ioports</code>, <code>/proc/net/*</code> 等等。</li>
<li><code>/sys</code>:这个目录其实跟/proc非常类似，也是一个虚拟的文件系统，主要也是记录 核心与系统硬件信息较相关的信息。 包括目前已载入的核心模块与核心侦测到的硬件设备信息等等。这个目录同样不占硬盘容量喔！</li>
</ul>
</li>
<li>
<p>因为是所有系统默认的软件（distribution发布者提供的软件）都会放置到<code>/usr</code>下面，因此这个目录有点类似 Windows 系统的“C:\Windows\ （当中的一部份） + C:\Program files\”这两个目 录的综合体，系统刚安装完毕时，这个目录会占用最多的硬盘容量。一般来说，<code>/usr</code>的次目录 建议有下面这些：</p>
</li>
<li>
<p>FHS 要求必须要存在的目录:</p>
<ul>
<li><code>/usr/bin/</code>:所有一般用户能够使用的指令都放在这里！<code>/usr/bin</code> 与 <code>/bin</code> 是一模一样了！另外，FHS 要求在此目录下不应该有子目录！</li>
<li><code>/usr/lib/</code>:基本上，与 <code>/lib</code> 功能相同，所以 <code>/lib</code> 就是链接到此目录中的！</li>
<li><code>/usr/local/</code>:系统管理员在本机自行安装自己下载的软件（非 distribution 默认提供者），建议安装到此目录， 这样会比较便于管理。举例来说，你的 distribution 提供的软件较旧，你想安装较新的软件但又不想移除旧版，此时你可以将新版软件安装于<code>/usr/local/</code>目录下，可与原先的旧版软件有分别啦！ 你可以自行到<code>/usr/local</code>去看看，该目录下也是具有bin, etc, include, lib...的次目录喔！</li>
<li><code>/usr/sbin/</code>:非系统正常运行所需要的系统指令。最常见的就是某些网络服务器软件 的服务指令（daemon）啰！不过基本功能与 <code>/sbin</code> 也差不多， 因此目前 <code>/sbin</code> 就是链接到此目录中的。</li>
<li><code>/usr/share/</code>:主要放置只读架构的数据文件，当然也包括共享文件。在这个目录下放置的数据几乎是不分硬件架构均可读取的数据，因为几乎都是文字文件嘛！在此目录下常见的还有这些次目录：
<ul>
<li><code>/usr/share/man</code>：线上说明文档。</li>
<li><code>/usr/share/doc</code>：软件杂项的文件说明。</li>
<li><code>/usr/share/zoneinfo</code>：与时区有关的时区文件。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>FHS 建议可以存在的目录：</p>
<ul>
<li><code>/usr/games/</code>:与游戏比较相关的数据放置处。</li>
<li><code>/usr/include/</code>:c/c++等程序语言的文件开始（header）与包含档（include）放置处， 当我们以tarball方式 （*.tar.gz 的方式安装软件）安装某些数据时，会使用到里头的许多包含档喔！</li>
<li><code>/usr/libexec/</code>:某些不被一般使用者惯用的可执行文件或脚本（script）等等，都会放 置在此目录中。例如大部分的 X 窗口下面的操作指令， 很多都是放在此目录下的。</li>
<li><code>/usr/lib&lt;qual&gt;/</code>:与 <code>/lib&lt;qual&gt;/</code>功能相同，因此目前 <code>/lib&lt;qual&gt;</code> 就是链接到此目录中。</li>
<li><code>/usr/src/</code>:一般源代码建议放置到这里，src有source的意思。至于核心源代码则建议放置到<code>/usr/src/linux/</code>目录下。</li>
</ul>
</li>
<li>
<p>如果<code>/usr</code>是安装时会占用较大硬盘容量的目录，那么<code>/var</code>就是在系统运行后才会渐渐占用硬盘容量的目录。 因为<code>/var</code>目录主要针对常态性变动的文件，包括高速缓存（cache）、登录文件 （log file）以及某些软件运行所产生的文件， 包括程序文件（lock file, run file），或者例如 MySQL数据库的文件等等。常见的次目录有：</p>
</li>
<li>
<p>FHS 要求必须要存在的目录:</p>
<ul>
<li><code>/var/cache/</code>:应用程序本身运行过程中会产生的一些暂存盘；</li>
<li><code>/var/lib/</code>:程序本身执行的过程中，需要使用到的数据文件放置的目录。在此目录下各自的软件应该要有各自的目录。举例来说，MySQL的数据库放置 到<code>/var/lib/mysql/</code>而rpm的数据库则放到<code>/var/lib/rpm</code>去！</li>
<li><code>/var/lock/</code>:某些设备或者是文件资源一次只能被一个应用程序所使用，如果同时有两个程序使用该设备时，就可能产生一些错误的状况，因此就得要将该设备上锁（lock），以确保该设备只会给单一软件所使用。目前此目录也已经挪到 <code>/run/lock</code>中！</li>
<li><code>/var/log/</code>:重要到不行！这是登录文件放置的目录！里面比较重要的文件 如<code>/var/log/messages</code>, <code>/var/log/wtmp</code>（记录登陆者的信息）等。</li>
<li><code>/var/mail/</code>:放置个人电子邮件信箱的目录，不过这个目录也被放置到<code>/var/spool/mail/</code> 目录中！ 通常这两个目录是互为链接文件啦！</li>
<li><code>/var/run/</code>:某些程序或者是服务启动后，会将他们的PID放置在这个目录下喔！至于 PID的意义我们会在后续章节提到的。与 <code>/run</code> 相同，这个目录链接到 <code>/run</code> 去了！</li>
<li><code>/var/spool/</code>:这个目录通常放置一些伫列数据，所谓的“伫列”就是排队等待其他程序使用的数据啦！ 这些数据被使用后通常都会被删除。举例来说，系统收到新信会放置到<code>/var/spool/mail/</code>中， 但使用者收下该信件后该封信原则上就会 被删除。信件如果暂时寄不出去会被放到<code>/var/spool/mqueue/</code>中， 等到被送出后就被删除。如果是工作调度数据（crontab），就会被放置到<code>/var/spool/cron/</code>目录中！</li>
</ul>
</li>
<li>
<h2 id="13-文件与目录管理">1.3 文件与目录管理</h2>
<ul>
<li>
<p>网络文件常常提到类似“./run.sh”之类的数据，这个指令的意义为何？答：由于指令的执行需要变量（bash章节才会提到）的支持，若你的可执行文件放置在本目录，并且本目录并非正规的可执行文件目录（/bin, /usr/bin等为正规），此时要执行指令就得要严格指定该可执行文件。“./”代表“本目录”的意思，所以“./run.sh”代表“执行本目录下，名为run.sh的文件”。</p>
</li>
<li>
<p>cd （change directory, 变换目录）</p>
<ul>
<li><code>cd ~dmtsai</code>: 代表去到 dmtsai 这个使用者的主文件夹，亦即 <code>/home/dmtsai</code></li>
<li><code>cd ~</code>:表示回到自己的主文件夹，亦即是 <code>/root</code> 这个目录</li>
<li><code>cd</code>:没有加上任何路径，也还是代表回到自己主文件夹的意思</li>
<li><code>cd ..</code>:表示去到目前的上层目录</li>
<li><code>cd -</code>:表示回到刚刚的那个目录</li>
<li><code>cd ../postfix</code>:由<code>/var/spool/mail</code> 去到<code>/var/spool/postfix</code></li>
</ul>
</li>
<li>
<p>一登陆Linux系统后，每个帐号都会在自己帐号的主文件夹中(<code>/home</code>)。</p>
</li>
<li>
<p>pwd （显示目前所在的目录）</p>
<ul>
<li><code>pwd [-P]</code>😛 ：显示出确实的路径，而非使用链接 （link） 路径。加上 <code>pwd -P</code> 的选项后，会不以链接文件的数据显示，而是显示正确的完整路径。</li>
<li><code>pwd</code>:单纯显示出目前的工作目录。</li>
</ul>
</li>
<li>
<p>mkdir （创建新目录）</p>
<ul>
<li><code>mkdir [-mp] 目录名称</code>:
<ul>
<li><code>-m</code> ：设置文件的权限喔！直接设置，不需要看默认权限 （umask） 的脸色～</li>
<li><code>-p</code> ：帮助你直接将所需要的目录（包含上层目录）递回创建起来！</li>
<li>示例:
<ul>
<li><code>mkdir test1/test2/test3/test4</code>:错误。mkdir: cannot create directory ‘test1/test2/test3/test4’: No such file or directory</li>
<li><code>mkdir -p test1/test2/test3/test4</code>:正确。</li>
</ul>
</li>
</ul>
</li>
<li>rmdir （删除“空”的目录）
<ul>
<li><code>rmdir [-p] 目录名称</code>:
<ul>
<li><code>-p</code> ：连同“上层”“空的”目录也一起删除。</li>
</ul>
</li>
<li>示例:
<ul>
<li><code>rmdir test1</code>:rmdir: failed to remove ‘test1’: Directory not empty</li>
<li><code>rmdir -p test1/test2/test3/test4</code>:利用 <code>-p</code> 这个选项，立刻就可以将 <code>test1/test2/test3/test4</code> 一次删除,包括test1</li>
<li>如果要将非空目录下的东西都杀掉呢？！ 这个时候就必须使用<code>rm -r 目录名称</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>文件与目录的检视： ls</p>
<ul>
<li><code>ls [-aAdfFhilnrRSt] 文件名或目录名称</code>
<ul>
<li><code>-a</code> ：全部的文件，连同隐藏文件（ 开头为 . 的文件） 一起列出来（常用）</li>
<li><code>-d</code> ：仅列出目录本身，而不是列出目录内的文件数据（常用）</li>
<li><code>-l</code> ：长数据串行出，包含文件的属性与权限等等数据（常用）</li>
<li>示例：
<ul>
<li><code>ls -al ~</code>: 将主文件夹下的所有文件列出来（含属性与隐藏文件）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>cp （复制文件或目录）</p>
<ul>
<li><code>cp [-adfilprsu] 来源文件（source） 目标文件（destination）</code></li>
<li><code>cp [options] source1 source2 source3 .... directory</code>
<ul>
<li><code>-a</code> ：相当于 -dr --preserve=all 的意思，至于 dr 请参考下列说明；（常用）</li>
<li><code>-i</code> ：若目标文件（destination）已经存在时，在覆盖时会先询问动作的进行（常用）</li>
<li><code>-r</code> ：递回持续复制，用于目录的复制行为；（常用）</li>
<li><code>-p</code> ：连同文件的属性（权限、用户、时间）一起复制过去，而非使用默认属性（备份常用）；</li>
<li><code>--preserve=all</code> ：除了 -p 的权限相关参数外，还加入 SELinux 的属性, links, xattr 等也复制了。</li>
<li>如果来源文件有两个以上，则最后一个目的文件一定要是“目录”才行！</li>
<li>如果目标目录或者目标文件不存在，则相当于重命名。</li>
<li>示例
<ul>
<li><code>cp ~/.bashrc /tmp/bashrc</code>:将主文件夹下的 .bashrc 复制到 <code>/tmp</code> 下，并更名为 bashrc</li>
<li><code>cp -i ~/.bashrc /tmp/bashrc</code>:cp: overwrite <code>/tmp/bashrc'? n不覆盖，y为覆盖，重复作两次动作，由于</code>/tmp<code>下面已经存在 bashrc 了，加上</code>-i` 选项后，则在覆盖前会询问使用者是否确定！可以按下 n 或者 y 来二次确认</li>
<li><code>cp /var/log/wtmp .</code>:复制到当前目录</li>
<li><code>cp /etc/ /tmp</code>:复制 <code>/etc/</code> 这个目录下的所有内容到 <code>/tmp</code> 下面，cp: omitting directory <code>/etc</code>，如果是目录则不能直接复制，要加上 <code>-r</code> 的选项</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>rm （移除文件或目录）</p>
<ul>
<li><code>rm [-fir] 文件或目录</code>：
<ul>
<li><code>-f</code> ：就是 force 的意思，忽略不存在的文件，不会出现警告讯息；</li>
<li><code>-r</code> ：递回删除啊！最常用在目录的删除了！这是非常危险的选项！！！</li>
<li><code>-rf</code>: 如子目录里面还有子目录时，那就要使用 -r 这个选项</li>
<li><code>-rf/</code>: 会将系统文件全部删除，非常危险  <s>删库跑路</s></li>
<li>示例：
<ul>
<li><code>rm bashrc*</code>:通过万用字符*的帮忙，将<code>/tmp</code>下面开头为bashrc的文件名通通删除</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>mv （移动文件与目录，或更名）</p>
<ul>
<li><code>mv [-fiu] source destination</code></li>
<li><code>mv [options] source1 source2 source3 .... directory</code>
<ul>
<li><code>-f</code> ：force 强制的意思，如果目标文件已经存在，不会询问而直接覆盖；</li>
<li><code>-i</code> ：若目标文件 （destination） 已经存在时，就会询问是否覆盖！</li>
<li><code>-u</code> ：若目标文件已经存在，且 source 比较新，才会更新 （update）</li>
<li>示例:
<ul>
<li>复制一文件，创建一目录，将文件移动到目录中：
<ul>
<li><code>cd /tmp</code></li>
<li><code>cp ~/.bashrc bashrc</code></li>
<li><code>mkdir mvtest</code></li>
<li><code>mv bashrc mvtest</code></li>
</ul>
</li>
<li>文件改名：
<ul>
<li><code>mv mvtest mvtest2</code></li>
</ul>
</li>
<li>再创建两个文件，再全部移动到 <code>/tmp/mvtest2</code> 当中:
<ul>
<li><code>cp ~/.bashrc bashrc1</code></li>
<li><code>cp ~/.bashrc bashrc2</code></li>
<li><code>mv bashrc1 bashrc2 mvtest2</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="二-pip">二、pip</h1>
<ul>
<li><code>pip install --upgrade pip</code>或<code>pip install -U pip</code>:升级pip自身</li>
<li><code>pip list</code>：查看已经通过pip安装的包</li>
<li><code>pip install &lt;包名&gt; --upgrade</code>或<code>pip install -U &lt;包名&gt;</code>:升级包</li>
<li><code>pip install &lt;包名&gt; -i https://mirrors.aliyun.com/pypi/simple</code>:指定单次安装源</li>
<li><code>pip install SomePackage==1.0.4</code>:指定版本</li>
<li><code>pip-review --auto/pip-review --local --interactive</code>:自动更新所有包</li>
</ul>
<h1 id="三-conda">三、conda</h1>
<ul>
<li><code>conda --version</code>:查看conda版本，验证是否安装</li>
<li><code>conda update conda</code>:更新至最新版本，也会更新其它相关包</li>
<li><code>conda update --all</code>:更新所有包</li>
<li><code>conda update package_name</code>:更新指定的包</li>
<li><code>conda create -n env_name package_name</code>:创建名为 env_name 的新环境，并在该环境下安装名为 package_name 的包，可以指定新环境的版本号，例如：<code>conda create -n python2 python=python2.7 numpy pandas</code>，创建了python2环境，python版本为2.7，同时还安装了numpy pandas包</li>
<li><code>conda activate env_name</code>:切换至env_name环境</li>
<li><code>conda deactivate</code>:退出环境</li>
<li><code>conda info -e</code>:显示所有已经创建的环</li>
<li><code>conda create -n new_env_name --clone old_env_name</code>:复制old_env_name为new_env_name</li>
<li><code>conda remove -n env_name –-all</code>:删除环境</li>
<li><code>conda list</code>:查看所有已经安装的包</li>
<li><code>conda install -n env_name package_name</code>:在指定环境中安装包</li>
<li><code>conda remove -n env_name package</code>:删除指定环境中的包</li>
<li><code>rm -rf 虚拟环境所在路径</code>:删除空环境</li>
</ul>
<h1 id="四-vim">四、Vim</h1>
<ul>
<li><code>shift+g</code>:移至文件末尾</li>
</ul>
<h1 id="五-其他">五、其他</h1>
<ul>
<li><code>sudo nautilus /home</code>:打开图形化文件夹</li>
<li><code>top</code>或<code>htop</code>：查看当前进程</li>
<li><code>ifconfig</code>：查看IP地址</li>
<li><code>kill 进程号</code>:终止进程</li>
<li><code>kill -9 进程号</code>:强制终止进程</li>
<li><code>tar -zxvf xxx.tar.gz</code>:解压缩</li>
<li><code>chmod u+x file</code>:对文件 file 增加文件主可执行权限（u表示文件主）</li>
<li><code>sudo apt-get install package</code>:安装包</li>
<li><code>sudo apt-get install package --reinstall</code>:重新安装包</li>
<li><code>sudo apt-get -f install</code>:修复安装</li>
<li><code>sudo apt-get remove package</code>:删除包</li>
<li><code>sudo apt-get remove package --purge</code>:删除包，包括删除配置文件等</li>
<li><code>sudo apt-get update</code>:更新源</li>
<li><code>sudo apt-get upgrade</code>:更新已安装的包</li>
<li><code>sudo apt-get dist-upgrade</code>:`升级系统</li>
<li><code>sudo apt-get autoclean</code>:清理旧版本软件缓存</li>
<li><code>sudo dpkg -i package.deb</code>:所有deb文件的安装</li>
<li><code>sudo dpkg -r package</code>：所有deb文件的卸载</li>
<li><code>sudo dpkg -P package</code>:彻底的卸载，包括软件的配置文件</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello World!]]></title>
        <id>https://bailingnan.github.io/post/hello-world</id>
        <link href="https://bailingnan.github.io/post/hello-world">
        </link>
        <updated>2019-12-22T17:10:09.000Z</updated>
        <content type="html"><![CDATA[<p>New begin!</p>
]]></content>
    </entry>
</feed>