<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://bailingnan.github.io/</id>
    <title>白凌南</title>
    <updated>2019-12-22T17:34:48.168Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://bailingnan.github.io/"/>
    <link rel="self" href="https://bailingnan.github.io//atom.xml"/>
    <subtitle>DL/RecSys/Python/Java/INTJ</subtitle>
    <logo>https://bailingnan.github.io//images/avatar.png</logo>
    <icon>https://bailingnan.github.io//favicon.ico</icon>
    <rights>All rights reserved 2019, 白凌南</rights>
    <entry>
        <title type="html"><![CDATA[Attention机制总结]]></title>
        <id>https://bailingnan.github.io//post/attention-ji-zhi-zong-jie</id>
        <link href="https://bailingnan.github.io//post/attention-ji-zhi-zong-jie">
        </link>
        <updated>2019-12-22T17:32:11.000Z</updated>
        <content type="html"><![CDATA[<!-- more -->
<p>Attention</p>
<p>在哪里体现了Attention呢？答案是: c_{i}，这个 Context Vector，上下文向量。<br>
区别于 Seq2Seq 直接把最后一个时序 i 的输出 h_{i} 作为上下文向量，而是将之前所有时序的输出通过加权求和得到的一个上下文向量。这个 c_{i} 包含着各个时序输出的权重信息，也就相当于告诉我们哪一段文字对于当前的 target word 是重要的，哪些是不重要的。这就相当于告诉我们的注意力应该放在哪里。<br>
c_{i} 是 Attention 矩阵的一行，表示输入 x_{1} 到 x_{t} 分别对 decoder 第 i 时序这个 target word 所对应的的权重（注意力大小）。<br>
\alpha_{ij} 是 Attention 矩阵的一个值，表示输入表示 x_{j} 对 decoder 第 i 时序的 target word 的权重。<br>
Attention 矩阵的计算方法如红框所示，由一个线性层和 softmax 层叠加得到。其输入是上一时刻 decoder 的状态 s_{i-1} 和 encoder 对第 j 个词的输出 h_{j} ， W_{a} 、 U_{a} 和 v_{a} 则是需要我们模型去学习的参数。其对应的物理意义就是：句子中某个词对应正准备翻译的词的权重（重要性），由词语本身（ h_{j} ）和前一个翻译的词（ s_{i-1} ）决定。<br>
Pytorch Tutorial神经网络翻译代码分析</p>
<p>使用前一步解码器的状态，先初始化：<br>
encoder_hidden = encoder.initHidden()<br>
decoder_hidden = encoder_hidden<br>
先进行attention的计算，再进入GRU，是Bahdanau等人的方式，有别于Luong Attention：<br>
class AttnDecoderRNN(nn.Module):<br>
def <strong>init</strong>(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):<br>
super(AttnDecoderRNN, self).<strong>init</strong>()<br>
self.hidden_size = hidden_size<br>
self.output_size = output_size<br>
self.dropout_p = dropout_p<br>
self.max_length = max_length</p>
<pre><code>    self.embedding = nn.Embedding(self.output_size, self.hidden_size)
    self.attn = nn.Linear(self.hidden_size * 2, self.max_length)
    self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)
    self.dropout = nn.Dropout(self.dropout_p)
    self.gru = nn.GRU(self.hidden_size, self.hidden_size)
    self.out = nn.Linear(self.hidden_size, self.output_size)

def forward(self, input, hidden, encoder_outputs):
    embedded = self.embedding(input).view(1, 1, -1)
    embedded = self.dropout(embedded)
    #embedded维度=[max_len(1,一个单词),batchsize(1,一个单词),hiddensize)
    #hidden维度=[num_layers(1),batchsize(1),hiddensize)
    attn_weights = F.softmax(
        self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)#[1,10]
    #encoder_outputs维度=[max_len,hiddensize]
    attn_applied = torch.bmm(attn_weights.unsqueeze(0),
                             encoder_outputs.unsqueeze(0))

    output = torch.cat((embedded[0], attn_applied[0]), 1)
    output = self.attn_combine(output).unsqueeze(0)

    output = F.relu(output)
    output, hidden = self.gru(output, hidden)

    output = F.log_softmax(self.out(output[0]), dim=1)
    return output, hidden, attn_weights

def initHidden(self):
    return torch.zeros(1, 1, self.hidden_size, device=device)
</code></pre>
<p>Pytorch Tutorial聊天机器人代码分析<br>
调用代码：<br>
model_name = 'cb_model'<br>
attn_model = 'dot'</p>
<h1 id="attn_model-general">attn_model = 'general'</h1>
<h1 id="attn_model-concat">attn_model = 'concat'</h1>
<p>hidden_size = 500<br>
encoder_n_layers = 2<br>
decoder_n_layers = 2<br>
dropout = 0.1<br>
batch_size = 64<br>
encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)<br>
decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)<br>
Encoder部分：<br>
输入:<br>
input_seq：一批输入句子; shape =（max_length，batch_size）<br>
input_lengths：一批次中每个句子对应的句子长度列表;shape=(batch_size)<br>
hidden:隐藏状态; shape =(n_layers x num_directions，batch_size，hidden_size)<br>
输出：<br>
outputs：GRU最后一个隐藏层的输出特征（双向输出之和）; shape =（max_length，batch_size，hidden_size）<br>
hidden：从GRU更新隐藏状态; shape =（n_layers x num_directions，batch_size，hidden_size）<br>
代码：<br>
class EncoderRNN(nn.Module):<br>
def <strong>init</strong>(self, hidden_size, embedding, n_layers=1, dropout=0):<br>
super(EncoderRNN, self).<strong>init</strong>()<br>
self.n_layers = n_layers<br>
self.hidden_size = hidden_size<br>
self.embedding = embedding</p>
<pre><code>    # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'
    #   because our input size is a word embedding with number of features == hidden_size
    self.gru = nn.GRU(hidden_size, hidden_size, n_layers,
                      dropout=(0 if n_layers == 1 else dropout), bidirectional=True)

def forward(self, input_seq, input_lengths, hidden=None):
    # Convert word indexes to embeddings
    embedded = self.embedding(input_seq)
    # Pack padded batch of sequences for RNN module
    packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)
    # Forward pass through GRU
    outputs, hidden = self.gru(packed, hidden)
    # Unpack padding
    outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)
    # Sum bidirectional GRU outputs
    outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]
    # Return output and final hidden state
    #hidden的维度=[num_layers*2,batchsize,hiddensize]
    #outputs的维度=[max_len,batchsize,hiddensize]
    return outputs, hidden
</code></pre>
<p>Decoder部分：<br>
通过“Global attention”，我们仅使用当前步的解码器的隐藏状态来计算注意力权重（或者能量）。 Bahdanau等人的注意力计算需要知道前一步中解码器的状态。 此外，Luong等人提供各种方法来计算编码器输出和解码器输出之间的注意权重（能量），称之为“score functions”：</p>
<p>其中 h_{t} = 当前目标解码器状态， \bar{h}_{s} = 所有编码器状态。<br>
计算注意力权重：用的是所有编码器状态encoder_outputs和经过GRU后的outputs<br>
以general为例：（其实不是很能理解这种权重计算方式</p>
<h1 id="luong-attention-layer">Luong attention layer</h1>
<p>class Attn(torch.nn.Module):<br>
def <strong>init</strong>(self, method, hidden_size):<br>
super(Attn, self).<strong>init</strong>()<br>
self.method = method<br>
if self.method not in ['dot', 'general', 'concat']:<br>
raise ValueError(self.method, &quot;is not an appropriate attention method.&quot;)<br>
self.hidden_size = hidden_size<br>
if self.method == 'general':<br>
self.attn = torch.nn.Linear(self.hidden_size, hidden_size)<br>
elif self.method == 'concat':<br>
self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)<br>
self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))</p>
<pre><code>def dot_score(self, hidden, encoder_output):
    return torch.sum(hidden * encoder_output, dim=2)

def general_score(self, hidden, encoder_output):
    #hidden维度=[num_layers,batchsize,hiddensize]
    energy = self.attn(encoder_output)
    #energy维度=[max_len,batchsize,hiddensize]
    return torch.sum(hidden * energy, dim=2)

def concat_score(self, hidden, encoder_output):
    energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()
    return torch.sum(self.v * energy, dim=2)

def forward(self, hidden, encoder_outputs):
    #hidden维度=[1,batchsize,hiddensize],步长为１
    #output维度=[max_len,batchsize,hiddensize]
    # Calculate the attention weights (energies) based on the given method
    if self.method == 'general':
        attn_energies = self.general_score(hidden, encoder_outputs)
    #维度=[max_len,batchsize]
    elif self.method == 'concat':
        attn_energies = self.concat_score(hidden, encoder_outputs)
    elif self.method == 'dot':
        attn_energies = self.dot_score(hidden, encoder_outputs)

    # Transpose max_length and batch_size dimensions
    attn_energies = attn_energies.t()#[batchsize,max_len]

    # Return the softmax normalized probability scores (with added dimension)
    return F.softmax(attn_energies, dim=1).unsqueeze(1)#[batchsize,1,max_len]
</code></pre>
<p>Luong Attention的实现：<br>
输入:<br>
input_step：每一步输入序列批次（一个单词）; shape =（1，batch_size）<br>
last_hidden：GRU的最终隐藏层; shape =（n_layers x num_directions，batch_size，hidden_size）<br>
encoder_outputs：编码器模型的输出; shape =（max_length，batch_size，hidden_size）<br>
输出:<br>
output: 一个softmax标准化后的张量， 代表了每个单词在解码序列中是下一个输出单词的概率; shape =（batch_size，voc.num_words）<br>
hidden: GRU的最终隐藏状态; shape =（n_layers x num_directions，batch_size，hidden_size）<br>
class LuongAttnDecoderRNN(nn.Module):<br>
def <strong>init</strong>(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):<br>
super(LuongAttnDecoderRNN, self).<strong>init</strong>()</p>
<pre><code>    # Keep for reference
    self.attn_model = attn_model
    self.hidden_size = hidden_size
    self.output_size = output_size
    self.n_layers = n_layers
    self.dropout = dropout

    # Define layers
    self.embedding = embedding
    self.embedding_dropout = nn.Dropout(dropout)
    self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))
    self.concat = nn.Linear(hidden_size * 2, hidden_size)
    self.out = nn.Linear(hidden_size, output_size)

    self.attn = Attn(attn_model, hidden_size)

def forward(self, input_step, last_hidden, encoder_outputs):
    # Note: we run this one step (word) at a time
    # Get embedding of current input word
    embedded = self.embedding(input_step)
    embedded = self.embedding_dropout(embedded)
    # Forward through unidirectional GRU
    rnn_output, hidden = self.gru(embedded, last_hidden)
    # Calculate attention weights from the current GRU output
    attn_weights = self.attn(rnn_output, encoder_outputs)
    # Multiply attention weights to encoder outputs to get new &quot;weighted sum&quot; context vector
    #权重维度＝[batchsize,1,max_len],encoder_outputs维度=[max_len,batchsize,hiddensize]
    context = attn_weights.bmm(encoder_outputs.transpose(0, 1))
    #context维度[batchsize,1,hiddensize]
    # Concatenate weighted context vector and GRU output using Luong eq. 5
    rnn_output = rnn_output.squeeze(0)
    context = context.squeeze(1)
    concat_input = torch.cat((rnn_output, context), 1)
    concat_output = torch.tanh(self.concat(concat_input))
    # Predict next word using Luong eq. 6
    output = self.out(concat_output)
    output = F.softmax(output, dim=1)
    # Return output and final hidden state
    return output, hidden
</code></pre>
<p>师兄代码：<br>
class RNN(nn.Module):<br>
def <strong>init</strong>(self, liner_size, n_units, cgi_dim, week_dim, hour_dim, cls_dim, cgi_size, batchsize, cls_ = 1):<br>
super(RNN, self).<strong>init</strong>()<br>
self.hidden_size = n_units<br>
self.batchsize = batchsize<br>
# self.add_module(&quot;batchnorm&quot;,nn.BatchNorm1d(n_units+em_dim))<br>
# self.add_module(&quot;lstm&quot;,nn.LSTM(liner_size + hour_dim + week_dim + cgi_dim, self.hidden_size, 2, dropout = dropout, batch_first = True))<br>
self.add_module(&quot;lstm&quot;,nn.LSTM(self.hidden_size, self.hidden_size, 2, batch_first = True))<br>
self.add_module(&quot;line_out&quot;,nn.Linear(self.hidden_size, cls_))<br>
self.hour_embedding = nn.Embedding(24, hour_dim)<br>
self.cgi_embedding = nn.Embedding(cgi_size, cgi_dim)<br>
self.week_embedding = nn.Embedding(7, week_dim)<br>
self.cls_embedding = nn.Embedding(2, cls_dim)<br>
self.hidden = self.init_hidden_state(self.batchsize)<br>
self.net = nn.Sequential(nn.Linear(1,liner_size), nn.PReLU())<br>
self.net2 = nn.Sequential(nn.Linear(liner_size + hour_dim, self.hidden_size), nn.PReLU())<br>
self.attn = nn.Sequential(nn.Tanh(), nn.Linear(self.hidden_size, 1, bias= False))<br>
#self.attn = nn.Sequential(nn.Linear(self.hidden_size, 1, bias= False))<br>
self.relu = nn.ReLU()</p>
<pre><code>def init_hidden_state(self, batchsize):
	return (Variable(torch.zeros(2,batchsize,self.hidden_size).cuda()),
			Variable(torch.zeros(2,batchsize,self.hidden_size).cuda()))


def forward(self, x, x_hour, x_cgi, x_week, x_cls, lookback): 
	x_s = self.net(torch.unsqueeze(x, 2))
	hour_ = self.hour_embedding(x_hour)
	#week_ = self.week_embedding(x_week)
	# cls_ = self.cls_embedding(x_cls)
	# print(x_s.size(),hour_.size(),cgi_.size(),week_.size())
	x_in = torch.cat((x_s, hour_),2)
	x_in = self.net2(x_in)
	# print(x_in.size())
	# pack = torch.nn.utils.rnn.pack_padded_sequence(x_in, lookback, batch_first=True)
	out , _ = self.lstm(x_in, self.hidden)#[128,32,64]
	#self.attn = nn.Sequential(nn.Tanh(), nn.Linear(self.hidden_size, 1, bias=False))
	attn_weights =  F.softmax(self.attn(out).transpose(1,2), dim = 2) # dim指定按对应维度计算softmax[128,1,32]
	context = torch.bmm(attn_weights, out)#[128,1,64]
            #线性层作decoder
	out_l = self.line_out(context)#[128,1,1]
	#out_l = self.relu(out_l)
	# unpacked = torch.nn.utils.rnn.pad_packed_sequence(out_l, batch_first = True)
	# y = unpacked[0]
	return out_l.squeeze()
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello World!]]></title>
        <id>https://bailingnan.github.io//post/hello-world</id>
        <link href="https://bailingnan.github.io//post/hello-world">
        </link>
        <updated>2019-12-22T17:10:09.000Z</updated>
        <content type="html"><![CDATA[<p>New begin!</p>
]]></content>
    </entry>
</feed>